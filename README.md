# Python Tutorial by 2021010668 ì‹ ë™í™˜

## 1. Dimensionality Reduction
<details>
    <summary> View Contents </summary>

  * ì°¨ì›ì¶•ì†Œì˜ ëª©ì : ë¶ˆí•„ìš”í•œ ë°ì´í„°ë¥¼ ì œê±°í•˜ì—¬ ë³µì¡ë„ë¥¼ ë‚®ì¶”ë©´ì„œ ì„±ëŠ¥ì„ ìœ ì§€, computation power ì ˆê°

  * ì°¨ì›ì¶•ì†Œ ê¸°ë²•ì˜ ì¢…ë¥˜
  ![image](https://user-images.githubusercontent.com/77199749/195138151-b06862d6-5887-42c5-b660-7b9d6816f127.png)

  * Genetic Algorithm
  ![image](https://user-images.githubusercontent.com/77199749/195138246-bb82f29a-2246-4db7-a469-3672852b8b72.png)

  * PCA vs. MDS
  ![image](https://user-images.githubusercontent.com/77199749/195138319-51de7065-8de3-4616-9dbb-e8a9b7548cc9.png)

</details>

## 2. SVM&SVR
<details>
    <summary> View Contents </summary>
    
  * Support Vector Machine(SVM): ë§ˆì§„ì„ ê·¹ëŒ€í™”í•˜ëŠ” ì´ˆí‰ë©´í™œìš© ë¶„ë¥˜ê¸°
  ![image](https://user-images.githubusercontent.com/77199749/195419556-907a0536-c7e5-4352-aecd-09477dc94630.png)

  * SVM 4 Cases
  ![image](https://user-images.githubusercontent.com/77199749/195419669-c1fcbe7a-acff-405e-923c-e78a1c3cc051.png)

  * SVM Case1: Linear&Hard Margin
  ![image](https://user-images.githubusercontent.com/77199749/195419851-e0627d18-2c57-452e-9665-a03dd14653a9.png)

  * Support Vector Regression: "ğ-tube"ë¥¼ ê°€ì§„ íšŒê·€ ì¶”ì •ì‹
  ![image](https://user-images.githubusercontent.com/77199749/195420076-533c14b2-9dbb-4476-a847-50e36ca8ff15.png)

</details>


## 3. Anomaly Detection
<details>
    <summary> View Contents </summary>
    
  * Anomaly Detection: "ì´ìƒì¹˜" íƒì§€ ê¸°ë²•
  
  * Novel Data: ì´ìƒì¹˜ ë°ì´í„°ì´ì§€ë§Œ, ê¸ì •ì ì¸ ì˜ë¯¸ë¡œ ì‚¬ìš©ë¨. ë°ì´í„°ì˜ ë³¸ì§ˆì  íŠ¹ì„±ì€ ê°™ì§€ë§Œ, ìœ í˜•ì´ ë‹¤ë¥¸ ê´€ì¸¡ì¹˜(ëŒì—°ë³€ì´ ë“±)
  * Anomaly Data: ì´ìƒì¹˜ ë°ì´í„°ì´ì§€ë§Œ, ë¶€ì •ì ì¸ ì˜ë¯¸ë¡œ ì‚¬ìš©ë¨.  ëŒ€ë¶€ë¶„ì˜ ë°ì´í„°ì™€ íŠ¹ì„±ì´ ë‹¤ë¥¸ ê´€ì¸¡ì¹˜(ë¶ˆëŸ‰ ë“±)
  * Outlier Data: ì´ìƒì¹˜ ë°ì´í„°, ëŒ€ë¶€ë¶„ì˜ ë°ì´í„°ì™€ ë³¸ì§ˆì ì¸ íŠ¹ì„±ì´ ë‹¤ë¥¸ ê´€ì¸¡ì¹˜(ë¯¸ì™„ì„±ì œí’ˆ ë“±)

  * Anomaly Detection vs Classification
    : ì•„ë˜ ê·¸ë¦¼ì²˜ëŸ¼, ì´ìƒì¹˜ íƒì§€ëŠ” ë°ì´í„°ì˜ ë¶ˆê· í˜•ì´ ì‹¬í•˜ê³  minority classì˜ ê°œìˆ˜ê°€ ì •ëŸ‰ì ìœ¼ë¡œ ë§¤ìš° ì ì„ ë•Œ ì‚¬ìš©í•œë‹¤.
  ![image](https://user-images.githubusercontent.com/77199749/201830267-fb474e19-cad8-43e5-baef-ef2061d4bcc4.png)

  * Density-based Anomaly Detection: ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê° ê°ì²´ë“¤ì´ ìƒì„±ë  í™•ë¥ ì„ ì¶”ì •í•˜ì—¬ ìƒˆë¡œìš´ ë°ì´í„°ê°€ ìƒì„±ë  í™•ë¥ ì´ ë‚®ì„ ê²½ìš° ì´ìƒì¹˜ë¡œ íŒë‹¨í•¨.

  * ì•„ë˜ì˜ 4ê°€ì§€ ë°€ë„ê¸°ë°˜ ì´ìƒì¹˜ íƒì§€ê¸°ë²•ë“¤ì„ ì†Œê°œí•¨
      1. Single Gassusian -> parametricê¸°ë²•ìœ¼ë¡œ,

          * ëª¨ë“  ë°ì´í„°ê°€ 'í•˜ë‚˜'ì˜ ê°€ìš°ì‹œì•ˆ(ì •ê·œ)ë¶„í¬ë¡œë¶€í„° ìƒì„±ë¨ì„ ê°€ì •í•¨.
          * í•™ìŠµì€ ì •ìƒë°ì´í„°ë“¤ì˜ ê°€ìš°ì‹œì•ˆë¶„í¬ì˜ í‰ê· (mu)ê³¼ ê³µë¶„ì‚°(var)í–‰ë ¬ì„ ì¶”ì •í•˜ë©° ì´ë£¨ì–´ì§
          * í…ŒìŠ¤íŠ¸ëŠ” ìƒˆë¡œìš´ ë°ì´í„°ê°€ ìƒì„±í™•ë¥ ì´ ë‚®ìœ¼ë©´ ì´ìƒì¹˜, ë†’ìœ¼ë©´ ì •ìƒìœ¼ë¡œ êµ¬ë¶„í•¨

          ![image](https://user-images.githubusercontent.com/77199749/201831498-e5f60dac-f6dd-48ca-8dfc-a87052a7b745.png)
          
          * ê²°ê³¼í•´ì„: Gaussian ê¸°ë²•ì€ ì¶”ì •ì´ ê°„ë‹¨í•˜ë©° í•™ìŠµê¸°ê°„ì´ ì§§ê³ , ì ì ˆí•œ threshold(epsilon)ë¥¼ ë¶„í¬ë¡œë¶€í„°ì •í•  ìˆ˜ ìˆë‹¤. ìœ„ ê·¸ë¦¼ì—ì„œ ë³´ì´ëŠ” ê²ƒì²˜ëŸ¼ epsilonê°’ì„ ì¤„ì¼ ìˆ˜ë¡ ì´ìƒì¹˜ì˜ ê°œìˆ˜(ë¹¨ê°„ìƒ‰)ê°€ ì¦ê°€í•˜ê³ , epsilonê°’ì„ ëŠ˜ë¦´ ìˆ˜ë¡ ì´ìƒì¹˜ì˜ ê°œìˆ˜(ë¹¨ê°„ìƒ‰)ê°€ ì¤„ì–´ë“œëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆì—ˆë‹¤.


      2. Gaussian Mixture -> parametricê¸°ë²•

          * ì—¬ëŸ¬ê°€ì§€ì˜ ê°€ìš°ì‹œì•ˆ(ì •ê·œ)ë¶„í¬ë¡œë¶€í„° ë°ì´í„°ê°€ ìƒì„±ë¬ìŒì„ ê°€ì •í•¨
          * Expectation-Maximization(EM)ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´, ê° ê°€ìš°ì‹œì•ˆë¶„í¬ì˜ ì¤‘ì‹¬ì (ì¡°ê±´í™•ë¥ )ì´ ëë‚ ë•Œê¹Œì§€ ë°˜ë³µí•¨

          ![image](https://user-images.githubusercontent.com/77199749/201831111-2db4a24c-176a-4217-a45d-0235391c3cad.png)
          
          * ê²°ê³¼í•´ì„: Mixture of Gaussian ê¸°ë²•ì€ Single Gaussianê¸°ë²•ì„ ë³´ì™„í•˜ì—¬, multi Gaussianë¶„í¬ë¥¼ ì¶”ì •í•´, ì¢€ ë” ìœ ì—°í•˜ê³  ë³µì¡í•œ boundaryë¥¼ ë§Œë“¤ ìˆ˜ ìˆê²Œ ëœë‹¤. ìœ„ ê·¸ë¦¼ì²˜ëŸ¼, 5ê°œì˜ í´ëŸ¬ìŠ¤í„°ê°€ ìˆì„ ë•Œì—ë„ ë‹¤ìˆ˜êµ°ì§‘ì˜ í™•ë¥ ë¶„í¬ë¥¼ êµ¬ë¶„í•  ìˆ˜ ìˆë„ë¡ ë§Œë“ ë‹¤. 

      3. Parzen Window Density Estimation -> non-parametricê¸°ë²•
          * ìœ„ ê°€ìš°ì‹œì•ˆê¸°ë²•ë“¤ì€ ë°ì´í„°ê°€ íŠ¹ì •ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•˜ì§€ë§Œ, íŒŒì   ìœˆë„ìš° ê¸°ë²•ì—ì„  ë°ì´í„°ê°€ íŠ¹ì •ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•˜ì§€ ì•ŠìŒ ==> non-parametricê¸°ë²•
          * í™•ë¥ ë°€ë„ë¥¼ ì¶”ì •í•  ë•Œ, ë°ì´í„°ì˜ ìˆ˜ëŠ” ê³ ì •ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì ì ˆí•œ V(ë¶€í”¼)ë¥¼ ì°¾ëŠ” ë¬¸ì œë¡œ ê·€ê²°ë˜ì§€ë§Œ, íŒŒì   ìœˆë„ìš° ê¸°ë²•ì—ì„  V(ë¶€í”¼)ë¥¼ ê³ ì •í•˜ê³ , "k(Vì•ˆì— í¬í•¨ë˜ëŠ” ë°ì´í„° ìˆ˜)"ë¥¼ ì°¾ìŒ ==> ì»¤ë„ ë°€ë„ ì¶”ì •

          ![image](https://user-images.githubusercontent.com/77199749/201831516-db5a7a33-4329-45b0-a5c7-875c601b031d.png)

          * ê²°ê³¼í•´ì„: ì»¤ë„ì„ í™œìš©í•œ íŒŒì  ìœˆë„ìš° ê¸°ë²•ì€ parametricí•œ ê°€ìš°ì‹œì•ˆê¸°ë²•ë“¤ê³¼ ë‹¤ë¥´ê²Œ, íŠ¹ì •ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•˜ì§€ ì•Šì•„ ë”ìš± ìœ ì—°í•˜ê³  ì„¬ì„¸í•œ boundaryë¥¼ ìƒì„± í•  ìˆ˜ ìˆë‹¤. íŠ¹ë³„íˆ, smoothing (window width) parameter hë¥¼ ì‚¬ìš©í•´ ë°€ë„ ë¶„í¬ë¥¼ ì¡°ì •í•  ìˆ˜ ìˆë‹¤.


      4. Local Outlier Factor(LoF) -> non-parametricê¸°ë²•
          * ì´ìƒì¹˜ ìŠ¤ì½”ì–´ë¥¼ ì‚°ì¶œ í• ë•Œ, "ì£¼ë³€ë¶€"ì˜ ë°€ë„ë„ ê°™ì´ ê³ ë ¤í•˜ì—¬ ì´ìƒì—¬ë¶€ë¥¼ ë‚˜íƒ€ë‚´ê³ ìí•¨
          * íŠ¹ì • ë°ì´í„° ì£¼ë³€ ë°€ë„ê°€ ì‘ì§€ë§Œ, ì£¼ë³€ë¶€ ë°ì´í„°ë“¤ì€ ë°€ë„ê°€ ë†’ì„ë•Œ --> ì´ìƒì¹˜ë¼ê³  íŒë‹¨í•¨

          ![image](https://user-images.githubusercontent.com/77199749/201831541-1e7cd125-89aa-4297-b7d6-4c9a65e60f43.png)
          
          * ê²°ê³¼í•´ì„: LoFëŠ” ìœ„ ì„¤ëª…ë“œë¦° 3ê°€ì§€ ê¸°ë²•ê³¼ ë‹¤ë¥´ê²Œ íŠ¹ì • ë°ì´í„°ì˜ 'ì£¼ë³€ë¶€ ë°€ë„'ë¥¼ ê³ ë ¤í•œë‹¤ëŠ”ì ì—ì„œ í° ì°¨ì´ì ì´ ìˆë‹¤. ìœ„ ê·¸ë¦¼ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, ê°€ì¥ìë¦¬ì— ìœ„ì¹˜í•˜ì—¬ ì£¼ë³€ë¶€ë°€ë„ê°€ ì‘ì€ ë°ì´í„°í¬ì¸íŠ¸ë“¤ì„ ì´ìƒì¹˜ë¼ê³  íŒë‹¨í•˜ëŠ”ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. íŠ¹ì • ì´ìƒì¹˜ë¥¼ ì •ì œí•˜ëŠ”ë°ì— ìˆì–´ì„  ì¢‹ì€ detectionë°©ì•ˆì´ ë  ìˆ˜ ìˆì„ê²ƒì´ë‹¤.


  * Model-based Anomaly Detection: ì£¼ì–´ì§„ ì •ìƒë°ì´í„°ë§Œìœ¼ë¡œ ê° ëª¨ë¸ë“¤ì„ í•™ìŠµí•˜ì—¬, ìƒˆë¡œìš´ ë°ì´í„°ê°€ ë“¤ì–´ì™”ì„ ë•Œ ê° ëª¨ë¸ì˜ ê¸°ì¤€ì— ë¶€í•©í•˜ì§€ ì•ŠëŠ” ë°ì´í„°ë“¤ì„ ì´ìƒì¹˜ë¼ íŒë‹¨í•¨.
  
  * ì•„ë˜ì˜ 3ê°€ì§€ ëª¨ë¸ê¸°ë°˜ ì´ìƒì¹˜ íƒì§€ê¸°ë²•ë“¤ì„ ì†Œê°œí•¨
    1. Auto-Encoder
        * ì´ë¯¸ì§€ ë°ì´í„°(ì˜ˆì‹œ)ë¥¼ ë„£ì—ˆì„ ë•Œ, ë˜‘ê°™ì€ ì´ë¯¸ì§€ë¥¼ ë³µì›í•´ë‚´ëŠ” NNëª¨ë¸
        * ì´ ë•Œ, ì •ìƒì¹˜ë§Œì„ í•™ìŠµì‹œì¼œ weightê°’ì„ ì €ì¥í•˜ê³  ìƒˆë¡œìš´ ì´ìƒì¹˜ê°€ ë“¤ì–´ì™”ì„ë•Œ ë³µì›errorê°’ì´ ë†’ì•„ì§€ë¯€ë¡œ, ë³µì›ì´ ì˜ ì•ˆë  ìˆ˜ë¡ ì´ìƒì¹˜ë¡œ íŒë‹¨
        
        ![image](https://user-images.githubusercontent.com/77199749/201831412-ec22f679-70cc-4d8b-8317-4211a7a14235.png)
        
        * ê²°ê³¼í•´ì„: Auto-encoderëŠ” ì¸ì½”ë”ë‹¨ì—ì„œ ì••ì¶•í•œ latent vectorë¥¼ decoderë‹¨ì—ì„œ ìƒì„±í•´ë‚´ëŠ” ëª¨ë¸ì´ë‹¤. ê·¸ë¦¼'outlier-score'ê·¸ë¦¼ì„ í†µí•´ ê·¹íˆ outlier-scoreê°€ ë†’ì€ ë°ì´í„°ë“¤ì„ í†µí•´ ì´ìƒì¹˜ë¥¼ íƒì§€í•  ìˆ˜ ìˆì—ˆë‹¤. ë˜í•œ 'Combination by average'ê·¸ë¦¼ì„ í†µí•´ 3ê°œì˜ ëª¨ë¸ì„ ë™ì‹œì— ì‚¬ìš©í•˜ê³  ì´ë¥¼ ì •ê·œí™”í•¨ìœ¼ë¡œì¨ ì¡°ê¸ˆ ë” ì •ì œëœ Anomaly_scoreë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.
    
    
    2. One Class Support Vector Machine(OCSVM)
        * Threshold(ì„ê³„ì¹˜)ê°€ ì•„ë‹Œ "boundary"ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ìƒì¹˜ ì—¬ë¶€ë¥¼ íŒë‹¨í•¨
        * OCSVMì€ ì›ì ìœ¼ë¡œë¶€í„°ì˜ ê±°ë¦¬ë“¤ì„ ì‚¬ìš©í•˜ì—¬ "ì´ˆí‰ë©´ boundary"ë¥¼ ë§Œë“¤ê³ , ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì´ìƒì¹˜ ì—¬ë¶€ë¥¼ íŒë‹¨
        (ì°¸ê³ )
        * SVDDê³¼ OCSVMì˜ ê³µí†µì ì€, ëª¨ë‘ threshold(ì„ê³„ì¹˜)ê°€ ì•„ë‹Œ boundaryë¥¼ ìƒì„±í•˜ì—¬ ì´ìƒì¹˜ì—¬ë¶€ë¥¼ êµ¬ë¶„í•˜ëŠ” ê²ƒì„
        * ì°¨ì´ì ì€, "boundaryì˜ í˜•íƒœì™€ ì¤‘ì‹¬ì "ì´ ë‹¤ë¦„
        * OCSVMì—ì„  ì´ˆí‰ë©´ì„ í™œìš©í•˜ì˜€ë‹¤ë©´, SVDDì—ì„  "ì´ˆêµ¬ boundary"ë¥¼ ì‚¬ìš©í•˜ë©° "ì›ì ì´ ì¤‘ì‹¬ì´ ì•„ë‹ˆì–´ë„ ë¬´ë°©"í•¨

        ![image](https://user-images.githubusercontent.com/77199749/201831461-7e44eced-05fe-4b6f-b4de-3f99e07a17b7.png)
        
        * ê²°ê³¼í•´ì„: ì¼ë°˜ì ì¸ ì„ í˜•ë¶„ë¥˜ê¸°ì¸ SVMê³¼ ë‹¤ë¥´ê²Œ ë¹„ì„ í˜•ì„±ì„ ë”í•œ OCSVMìœ¼ë¡œ ì›ì ìœ¼ë¡œë¶€í„° ë–¨ì–´ì§„ ê±°ë¦¬ë¡œ ì´ˆí‰ë©´ì„ ë§Œë“¤ê³  ë°ì´í„°ë“¤ì„ ì •ìƒ/ì´ìƒì¹˜ë¡œ ë¶„ë¥˜í•˜ëŠ” ì„±ëŠ¥ì´ ê·¸ë¦¼ì—ì„œì²˜ëŸ¼ ëˆˆì— ë ê²Œì¢‹ì•˜ë‹¤. ì—¬ëŸ¬ ë°ì´í„°ë“¤ ì†ì— í¬í•¨ë˜ì–´ ìˆëŠ” ë°ì´í„°ê°€ ì´ìƒì¹˜ì¼ë•, ë¶„ë¥˜í•˜ê¸° ì–´ë µì§€ë§Œ ìœ„ ê·¸ë¦¼ì²˜ëŸ¼ ê²½ê³„ë©´ì—ì„œ ë°œìƒí•˜ëŠ” ì´ìƒì¹˜ëŠ” thresholdë¥¼ ì •í•˜ì—¬ ë¶„ë¥˜ë¨ì„ ë³¼ ìˆ˜ ìˆë‹¤.

    3. Isolation Forest
        * Forestë¼ëŠ” ë‹¨ì–´ì—ì„œ ì•Œ ìˆ˜ ìˆë“¯ì´, ë¶„ê¸°ë¥¼ í•˜ë©´ì„œ ì´ìƒì¹˜ì—¬ë¶€ë¥¼ ë”°ì§€ëŠ” treeêµ¬ì¡°ì˜ ëª¨ë¸ì„
        * ì´ ë•Œ, ì†Œìˆ˜ ë²”ì£¼(ì´ìƒì¹˜)ëŠ” ê°œì²´ìˆ˜ê°€ ì ì„ ê²ƒì´ë¯€ë¡œ ì ì€ ë¶„ê¸°ë§Œìœ¼ë¡œ ê³ ë¦½ì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê°€ì •
        * ë”°ë¼ì„œ, ê³ ë¦½ì‹œí‚¤ëŠ”ë°ì— ë§ì€ ë¶„ê¸°ê°€ í•„ìš”í•˜ë©´ ì •ìƒ, ì ì€ ë¶„ê¸°ê°€ í•„ìš”í•˜ë‹¤ë©´ ì´ìƒì¹˜ë°ì´í„°ë¡œ íŒë‹¨í•¨

        ![image](https://user-images.githubusercontent.com/77199749/201831471-541147c9-d165-422a-bfd3-cdffad30a401.png)

        * ê²°ê³¼í•´ì„: Isolation ForestëŠ” Forestì˜ ì¥ì ì„ ì´ìƒì¹˜ íƒì§€ì— ì˜ ë…¹ì—¬ë‚´ì—ˆë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. ìœ„ ê·¸ë¦¼ì—ì„œ ì²˜ëŸ¼, ë¶„ê¸° ìˆ˜ê°€ ë§ì´ í•„ìš”ì—†ëŠ” ì´ìƒì¹˜ ë°ì´í„°ë“¤ì€ ì „ë¶€ ì´ìƒì¹˜ë¡œ ì˜ ë¶„ë¥˜ë˜ì–´ì§€ê³ , ë¶„ê¸°ê°€ ë§ì´ í•„ìš”í•œ, ì¦‰ ì •ìƒ ë°ì´í„°ë“¤ì€ ì •ìƒìœ¼ë¡œ ë¶„ë¥˜ë¨ì„ ë³¼ ìˆ˜ ìˆë‹¤. ì—¬ëŸ¬ê°œì˜ ë°€ì§‘ëœ êµ°ì§‘ì´ í˜•ì„±ë˜ì–´ ìˆì„ ë•Œ ì‚¬ìš©í•˜ë©´ íš¨ê³¼ì ì¸ ì´ìƒì¹˜ íƒì§€ ì„±ëŠ¥ì„ ë³¼ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.

  
</details>

## 4. Ensemble Learning
<details>
    <summary> View Contents </summary>
    
  * Ensemble: ê°œë³„ ì•Œê³ ë¦¬ì¦˜ë“¤ì„ ì ë‹¹íˆ ì˜ ì„ì–´ ë†“ìœ¼ë©´ ì–´ë–¨ê¹Œ? ë¼ëŠ” motivationì—ì„œ ì¶œë°œ
  
  * ëª¨ë¸í•™ìŠµ ë° ì˜¤ë¥˜
    * ëª¨ë¸ì´ë€, ì£¼ì–´ì§„ í•˜ë‚˜ì˜ ìƒ˜í”Œ ì§‘í•©ìœ¼ë¡œë¶€í„° ë°ì´í„° ìƒì„± í•¨ìˆ˜ë¥¼ ì¶”ë¡ í•˜ëŠ” ê²ƒì„
    * ëª¨ë¸ì— ì˜í•œ ì˜¤ë¥˜ëŠ” í¸í–¥(Bias)ì™€ ë¶„ì‚°(Variance)ë¡œ êµ¬ë¶„ ë  ìˆ˜ ìˆìŒ
    * í¸í–¥(Bias): í‰ê· ì ìœ¼ë¡œ ì–¼ë§ˆë‚˜ â€œì •í™•í•œâ€œ ì¶”ì •ì´ ê°€ëŠ¥í•œì§€ì— ëŒ€í•œ ì¸¡ì • ì§€í‘œ
    * ë¶„ì‚°(Variance): ê°œë³„ ì¶”ì •ì´ ì–¼ë§ˆë‚˜ â€œì°¨ì´â€ê°€ í¬ê²Œ ë‚˜íƒ€ë‚˜ëŠ”ì§€ì— ëŒ€í•œ ì¸¡ì • ì§€í‘œ
    
  * ëª¨ë¸ êµ¬ë¶„
    * ë¶„ì‚°ì´ ë‚®ê³ , í¸í–¥ì´ ë†’ìŒ --> Boosting
    * í¸í–¥ì´ ë‚®ê³ , ë¶„ì‚°ì´ ë†’ìŒ --> Bagging
    
    ![image](https://user-images.githubusercontent.com/77199749/204252620-980730d0-54b9-4714-8cbc-3caa5f279e79.png)




  * Bagging : Bootstrap Aggregatingì˜ ì•½ìë¡œ, ê°  ëª¨ë¸ì— ë‹¤ë¥¸ í•™ìŠµë°ì´í„° ì…‹ì„ ì´ìš©í•´ ê²°ê³¼ê°’ì„ ì·¨í•©í•˜ëŠ” ì•™ìƒë¸” ê¸°ë²•

  * ì•„ë˜ Baggingê¸°ë°˜ ì•™ìƒë¸” ê¸°ë²•ì„ ì†Œê°œí•¨
      1. Random Forest

          * Random Forestsë€, ë‹¤ìˆ˜ì˜ decision treeëª¨ë¸ì—
          * ë™ì¼í•˜ì§€ ì•Šì€ ë°ì´í„°ì…‹(í¬ê¸°ëŠ” ë™ì¼, ì¤‘ë³µê°€ëŠ¥)ì„ 'ë…ë¦½ì 'ìœ¼ë¡œ í•™ìŠµ ì‹œí‚¨ í›„
          * ê²°ê³¼ë¥¼ majority votingê³¼ ê°™ì´ ì·¨í•©í•˜ëŠ” ëª¨ë¸ì„
          * baggingì˜ ëŒ€í‘œì ì¸ ëª¨ë¸ì´ë¼ í•  ìˆ˜ ìˆìŒ
    
            ![image](https://user-images.githubusercontent.com/77199749/204252497-deeca3ea-1cbd-4974-8a6e-71cd70b96e96.png)


          
          * ê²°ê³¼í•´ì„: í•˜ë‚˜ì˜ decision treeë¡œëŠ” overfittingì´ ì‰½ê²Œ ì¼ì–´ë‚  ìˆ˜ ìˆì§€ë§Œ, ë‹¤ìˆ˜ì˜ treeëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê°ê¸° ë‹¤ë¥¸ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµì„ ì‹œì¼œ ì¼ë°˜í™”ëœ treeë¥¼ ë§Œë“¤ì—ˆìŒ. ì•™ìƒë¸” ëª¨ë¸ë¡œ, ì—¬ëŸ¬ê°œì˜ í˜•ì„±ëœ treeëª¨ë¸ì— ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ í†µê³¼ì‹œí‚¤ë©°, ê° íŠ¸ë¦¬ê°€ ë¶„ë¥˜í•œ ê²°ê³¼ì—ì„œ votingì„ ì‹¤ì‹œí•˜ì—¬ ê°€ì¥ ë§ì´ ë“í‘œí•œ ê²°ê³¼ë¥¼ ìµœì¢… ë¶„ë¥˜ ê²°ê³¼ë¡œ ì„ íƒí•¨. ë˜í•œ, Information Gain(IG)ê³¼ Gini impurityë¥¼ ê¸°ì¤€ìœ¼ë¡œ splitì„ ì§„í–‰í•˜ì—¬ ë¹„êµí•œ ê²°ê³¼, ë¶„ë¥˜ ì •í™•ë„ì™€ errorì¹´ìš´íŠ¸ìˆ˜ê°€ ë™ì¼í•œê²ƒìœ¼ë¡œ ë³´ì•„, ë¶“ê½ƒ ë°ì´í„°ì™€ ê°™ì´ ë‹¨ìˆœë°ì´í„°ì— ëŒ€í•´ì„  í° ì°¨ì´ê°€ ì—†ìŒì„ í™•ì¸í•˜ì˜€ë‹¤.


  * Boosting: Boostingì€ ì˜¤ë¶„ë¥˜ëœ ë°ì´í„°ì— ì§‘ì¤‘í•´ ë” ë§ì€ ê°€ì¤‘ì¹˜ë¥¼ ì£¼ëŠ” ensemble ê¸°ë²•ìœ¼ë¡œ, íŠ¹ì§•ì€, ê° ëª¨ë¸ì´ ì„œë¡œì—ê²Œ dependent(model guided)í•œ ì„±ì§ˆì„ ì§€ë…€ ë‹¤ì–‘ì„± ì¸¡ë©´ì—ì„œ explicití•¨
  
  * ì•„ë˜ì˜ 4ê°€ì§€ Boostingê¸°ë°˜ ì•™ìƒë¸” ëª¨ë¸ë“¤ì„ ì†Œê°œí•¨
    1. Adaptive Boosting(AdaBoost)
        * AdsBoostì—ì„  ì´ì „ë‹¨ê³„ì—ì„œì˜ ë‹¨ì ì„ ë°ì´í„°ë¶„í¬ì— ë°˜ì˜ì‹œí‚¤ë©´ì„œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚´.
        * ê°€ì¥ ê¸°ë³¸ì ì¸ AdabBoostì˜ base learner(estimator)ëŠ” depthê°€ 1ì¸ DecisionTreeì„
        * ë¶„ê¸°íšŸìˆ˜(depth)ë¥¼ ì¦ê°€ì‹œí‚¤ê³ ,
        * ì¶”ì •íšŸìˆ˜(n_estimators)ë¥¼ ì¦ê°€ì‹œí‚¤ë©´ì„œ ë¹„êµ ì‹¤í—˜í•˜ì˜€ìŒ
        
        ![image](https://user-images.githubusercontent.com/77199749/204252879-498089dc-f8f5-4e19-9223-7ab93108c6be.png)
        
        * ê²°ê³¼í•´ì„: AdaBoostë¥¼ í†µí•´ boostingì˜ ê¸°ë³¸ì ì¸ ê°œë…ì€ ì´ì „ ë‹¨ê³„ì—ì„œ ë§ì¶”ì§€ ëª»í•œ ë°ì´í„°ë“¤ì˜ ì˜í–¥ë ¥ì„ í‚¤ì›Œ ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ë” ì˜ ë§ì¶”ê²Œ ìœ ë„í•˜ëŠ” ê²ƒì„ì„ ì•Œì•˜ë‹¤. Treeê³„ì—´ì˜ ëª¨ë¸ì„ base modelë¡œ í• ë•Œì—ëŠ” ì£¼ë¡œ depth(leafë…¸ë“œê¹Œì§€ì˜ ë¶„ê¸°ìˆ˜ë¼ê³  ì´í•´í•˜ë©´ í¸í•¨)ë¥¼ ê¹Šê²Œí•˜ê³  ì¶”ì •íšŸìˆ˜ë¥¼ ë” ë§ì´ í•˜ëŠ”ê²ƒì´ ì„±ëŠ¥ì´ ì œì¼ ì¢‹ì€ ê²ƒì„ í™•ì¸í•˜ì˜€ë‹¤.
    
    
    2. Gradient Boosting Machine(GBM)
        * AdaBoostì™€ ë¹„ìŠ·í•œ boosting ê³„ì—´ì˜ ëª¨ë¸ì´ì§€ë§Œ,
        * GBMì—ì„  ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ë¥¼ Gradient Descentë¥¼ ì´ìš©í•´ íšŒê·€ëª¨í˜•ì˜ ì”ì°¨ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„
        * AdaBoostì—ì„  ì „ ë‹¨ê³„ì˜ ë‹¨ì ì´ 'ë°ì´í„°ì˜ ì„ íƒí™•ë¥ 'ì— ë°˜ì˜ë˜ì§€ë§Œ,
        * GBMì—ì„  'ì”ì°¨'ë¥¼ êµ¬í•˜ëŠ” 'ì†ì‹¤í•¨ìˆ˜ì˜ gradient'ì— ë°˜ì˜ë¨

        ![image](https://user-images.githubusercontent.com/77199749/204253044-f7194157-9d87-4f5d-876f-61097fed4654.png)
        
        * ê²°ê³¼í•´ì„: AdaBoostì™€ ë™ì¼í•œ ì¡°ê±´(max_depth = 10, n_estimator=100)ìœ¼ë¡œ ì§„í–‰í•˜ì˜€ì„ë•Œë³´ë‹¤ ì„±ëŠ¥ì´ '9%'ì´ìƒ í–¥ìƒëœ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ë‹¨ìˆœíˆ ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ë°”ê¾¸ì–´ê°€ë©° boostingì„ í•˜ëŠ” ê²ƒ ë³´ë‹¨, ì†ì‹¤í•¨ìˆ˜ì˜ gradientì— ë°˜ì˜í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ëŠ” ê²ƒì´ í•´ë‹¹ ë°ì´í„°ì—ì„  ë³´ë‹¤ íš¨ê³¼ì ì„ì„ ì•Œì•˜ë‹¤. ê·¸ëŸ¬ë‚˜, ì‹œê°„ì ì¸ ì¸¡ë©´ì„ ë³´ìë©´ AdaBoostì— ë¹„í•´ '8.8ë°° ì˜¤ë˜'ê±¸ë¦¬ëŠ” ê²ƒì€ ì¶©ë¶„íˆ ê³ ë ¤í•´ì•¼í•  ì‚¬í•­ì´ë‹¤. ì¶”ê°€ë¡œ, GBMë‚´ì—ì„œ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê³ ì treeì˜ depthë¥¼ 2ë°°ë¡œ ì¦ê°€ì‹œì¼œ 20ì„ ë§Œë“¤ê³  ë°˜ë³µíšŸìˆ˜ë¥¼ 100ì—ì„œ 50íšŒë¡œ ë‚®ì¶”ì–´ ì§„í–‰ì„ í•œ ê²°ê³¼, ì„±ëŠ¥ì´ ì˜¤íˆë ¤ ê°ì†Œë˜ì—ˆë‹¤. íŠ¹ì •ë°ì´í„°ì—ì„œ treeì˜ depthë¥¼ ë†’ê²Œ ì¡ëŠ” ê²ƒì´ ë” ì •í™•í•œ ë¶„ë¥˜ê°€ ê°€ëŠ¥í•œ ê²ƒì´ë¼ ì˜ˆìƒí•˜ì˜€ìœ¼ë‚˜, ê³¼ì í•©ìœ¼ë¡œ ì¸í•œ ì •í™•ë„ ê°ì†Œë¥¼ ë³´ì˜€ë‹¤.

    3. XGBoost
        * GBMì€ ìœ„ ì‹¤í—˜ì—ì„œ ì•Œ ìˆ˜ ìˆë“¯ì´, Adaboostë³´ë‹¤ 100ë°°ì˜ ì‹œê°„ì´ ê±¸ë¦¬ê³ , ê³¼ì í•©ì˜ ì´ìŠˆê°€ ìˆì—ˆìŒ
        * ì´ëŸ¬í•œ ë‹¨ì ì„ ë³´ì™„í•˜ê¸° ìœ„í•´ XGBoostê°€ íƒ„ìƒí•¨
        * ì „ì—­ì„ íƒìƒ‰í•œ ë¶„ê¸°ì ì´ ì•„ë‹ˆë¼, 'locally ìµœì splitì„ ì°¾ìœ¼ë¯€ë¡œ' GBM ë³´ë‹¤ ë¹ ë¥´ê³ 
        * ê³¼ì í•©ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ê·œì œë¥¼ í¬í•¨ì‹œí‚´

        ![image](https://user-images.githubusercontent.com/77199749/204253112-8b2834b8-9fb3-4a73-bd41-3843537f5f17.png)

        * AdaBoostì™€ ë™ì¼í•œ ì¡°ê±´(max_depth = 10, n_estimator=100)ìœ¼ë¡œ ì§„í–‰í•˜ì˜€ì„ë•Œë³´ë‹¤ ì„±ëŠ¥ì´ '7%' í–¥ìƒëœ ë°˜ë©´ GBMë³´ë‹¨ '2% ë‚®ì€ ì„±ëŠ¥'ì„ ë³´ì¸ë‹¤. GBMì²˜ëŸ¼ gradientì˜ ê°’ì„ ë°˜ì˜ì‹œì¼œ ì”ì°¨ë¥¼ ì¤„ì´ëŠ” ê²ƒì€ ë™ì¼í•˜ë‚˜,ì „ì—­ì„ íƒìƒ‰í•˜ì—¬ ë¶„ê¸°ì ì„ ì°¾ì§€ ì•Šê³ , ë¶„í• ëœ ë°ì´í„°ì…‹ì—ì„œ ìµœì ì˜ ë¶„ê¸°ì ì„ ì°¾ëŠ” ê²ƒì´ ì°¨ì´ì ì´ë‹¤. ì´ë¥¼ í†µí•´ XGBoostëŠ” GBMì— ë¹„í•´ '200ë°° ì´ìƒ ë¹ ë¥´ê²Œ' í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ëœë‹¤.    

    
    4. LightGBM
        * GBMì—ì„œ ì¶œë°œí•œ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ, ë‘ê°€ì§€ì˜ ê´€ì ì„ ì œì‹œí•œë‹¤
        * 1.Gradient-based One-slide Sampling(GOSS): ì •ë³´ëŸ‰ì´ ì ì€ê²ƒì€ ì œì™¸í•˜ê³  í•™ìŠµì„ ì§„í–‰í•˜ì
        * 2.Exclusive Feature Bundling(EFB): ë³€ìˆ˜ë“¤ì„ í•©ì¹˜ì
        * leaf-wise tree growthë¡œ, treeë¥¼ ìˆ˜í‰ì´ ì•„ë‹Œ 'ìˆ˜ì§'ìœ¼ë¡œ í™•ì¥í•˜ì—¬ leaf nodeì˜ ê°œìˆ˜ë¥¼ ì •í•¨
        * LightGBMì˜ ë¦¬í”„ ì¤‘ì‹¬ íŠ¸ë¦¬ ë¶„í•  ë°©ì‹ì€ íŠ¸ë¦¬ì˜ ê· í˜•ì„ ë§ì¶”ì§€ ì•Šê³ , ìµœëŒ€ ì†ì‹¤ ê°’(max delta loss)ì„ ê°€ì§€ëŠ”
        * ë¦¬í”„ ë…¸ë“œë¥¼ ì§€ì†ì ìœ¼ë¡œ ë¶„í• í•˜ë©´ì„œ íŠ¸ë¦¬ì˜ ê¹Šì´ê°€ ê¹Šì–´ì§€ê³  ë¹„ëŒ€ì¹­ì ì¸ ê·œì¹™ íŠ¸ë¦¬ê°€ ìƒì„±ë¨

        ![image](https://user-images.githubusercontent.com/77199749/204253228-215111e1-80da-4773-8399-75b1ea42bfc7.png)

        * ê²°ê³¼í•´ì„: AdaBoostì™€ ë™ì¼í•œ ì¡°ê±´(max_depth = 10, n_estimator=100)ìœ¼ë¡œ ì§„í–‰í•˜ì˜€ì„ë•Œë³´ë‹¤ ì„±ëŠ¥ì´ '9%'í–¥ìƒë˜ì—ˆìœ¼ë©°, GBMë³´ë‹¨ '0.2% ë‚®ì€ ì„±ëŠ¥'ìœ¼ë¡œ ê±°ì˜ ë¹„ìŠ·í•´ ë³´ì¸ë‹¤. GBMê³¼ gradientì˜ ê°’ì„ ë°˜ì˜ì‹œì¼œ ì”ì°¨ë¥¼ ì¤„ì´ëŠ” ê²ƒì€ ë™ì¼í•˜ë‚˜, lightë¼ëŠ” ì´ë¦„ì´ ë¶™ì€ ë§Œí¼ í”¼ì³ë¥¼ ë³‘í•©í•´ ë°ì´í„°í¬ê¸°ë¥¼ ì‘ê²Œ ë§Œë“¤ì–´ ë©”ëª¨ë¦¬ë¥¼ ì ê²Œ ì°¨ì§€í•˜ê³  ì´ë¥¼ í†µí•´ LightGBMì€\ëŠ” GBMì— ë¹„í•´ '170ë°° ì´ìƒ ë¹ ë¥´ê²Œ' í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ëœë‹¤. ì¶”ê°€ë¡œ, learning_rateë¥¼ 0.1ì—ì„œ 0.01ë¡œ ê°ì†Œí•œ ê²°ê³¼ë¥¼ ë³´ë©´ gradientì˜ ê°ì†Œê°€ ì²œì²œíˆ ì¼ì–´ë‚˜ ì„±ëŠ¥ì´ ë” ë–¨ì–´ì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. localì— ë¹ ì§ˆ ìš°ë ¤ê°€ ìˆì–´ ë‚®ì¶”ì—ˆìœ¼ë‚˜, 0.1ì´ ì´ë²ˆë°ì´í„°ì—” ë” ì ì ˆí•œ learning rateê°’ì„ì„ ì•Œ ìˆ˜ ìˆì—ˆë‹¤.

    5. CatBoost
        * GBMì—ì„œ ì¶œë°œí•œ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ, 'Categorical'ë°ì´í„°ì˜ boostingì„ ìˆ˜í–‰í•˜ëŠ” ëª¨ë¸ì„
        * Issue 1.Predcition Shift: í›„ë°˜ìœ¼ë¡œ ê°ˆìˆ˜ë¡ ì´ˆë°˜ì— ë§Œë“¤ì–´ì§„ ëª¨ë¸ì— ì˜í–¥ì„ ë§ì´ ë°›ëŠ” ë¬¸ì œê°€ ë°œìƒí•¨
            --> ê·¸ ì´ìœ ëŠ”, ë™ì¼í•œ ë°ì´í„°ê°€ ì—¬ëŸ¬ë²ˆ ì¤‘ë³µí•™ìŠµë˜ê¸° ë•Œë¬¸ì„.
            (Solution): Ordered Boosting: oblivious tree(ëŒ€ì¹­)êµ¬ì¶•í•˜ê³ , ì œí•œì ì¸ ë°ì´í„°ë¥¼ í•™ìŠµì— ì‚¬ìš©í•˜ê²Œ í•¨
                ì´ë–„, ê° ë°ì´í„°ë“¤ì— ëŒ€í•´ leaf valueì™€ gradientì— ëŒ€í•œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•´ lossë¡œ ì‚¬ìš©í•¨!!
        * Issue 2.Target Leakage: 2ë²ˆì§¸ ë¬¸ì œëŠ” ì¹´í…Œê³ ë¦¬ë³€ìˆ˜ë“¤ì€ one-hot encodingì„ í†µí•´ ë³€í™˜ì´ ë˜ëŠ”ë° yê°’ì€ ê·¸ëŒ€ë¡œ ìœ ì§€ë˜ëŠ” ë¬¸ì œê°€ ë°œìƒí•¨
            (Solution): Ordered Target Statistics: ì‚¬ì „í™•ë¥ ê°’ì„ ë„ì…í•œ ordered target statisticsë¥¼ ì‚¬ìš©í•˜ë©°,
                rareí•œ noisy categoriesë“¤ì˜ ë¶€ì •ì ì¸ íš¨ê³¼ë¥¼ ì œê±°í•¨

        ![image](https://user-images.githubusercontent.com/77199749/204253276-c0af524e-a05a-45a8-8ccb-f079e9ba6096.png)

       * ê²°ê³¼í•´ì„: AdaBoostì™€ ë™ì¼í•œ ì¡°ê±´(max_depth = 10, n_estimator=100)ìœ¼ë¡œ ì§„í–‰í•˜ì˜€ì„ë•Œë³´ë‹¤ ì„±ëŠ¥ì´ '6%' í–¥ìƒë˜ì—ˆìœ¼ë©°, GBMë³´ë‹¨ '3% ë‚®ì€ ì„±ëŠ¥'ì„ ë³´ì¸ë‹¤. GBMì˜ target leakageì™€ prediction shift ë¬¸ì œì ì„ ë³´ì™„í•œ ëª¨ë¸ì´ ë°”ë¡œ ì´ CatBoostì´ë‹¤. í•´ë‹¹ ë°ì´í„°ì—ì„  ê°’ë“¤ì´ ëª¨ë‘ 0ê³¼1ë¡œë§Œ ì´ë£¨ì–´ì ¸ ìˆê¸° ë•Œë¬¸ì— categoricalí•œ ë°ì´í„°ì„ì—ë„ ì„±ëŠ¥ì´ ë” ë‚®ê²Œ ë‚˜ì™”ìŒì„ ë³¼ ìˆ˜ ìˆë‹¤. ë°ì´í„°ì˜ ìˆ˜ê°€ ë§ê³  binaryê°’ì´ ì•„ë‹Œ  ë°ì´í„°ë¥¼ ì‹¤í—˜í•œ ê²°ê³¼ì—ì„  CatBoostì˜ ì„±ëŠ¥ì´ ë” ì¢‹ê²Œ ë‚˜ì„ ìˆ˜ ìˆë‹¤ê³  ê¸°ëŒ€í•´ë³¸ë‹¤.  ë§ˆì§€ë§‰ìœ¼ë¡œ, CatBoostëŠ” GBMì— ë¹„í•´ '15ë°° ì´ìƒ ë¹ ë¥´ê²Œ' í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ë˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.
    
    * ìµœì¢…ì‹¤í—˜ê²°ê³¼ë¹„êµ(ì •í™•ë„ ë†’ì€ìˆœ, base_learner: tree, max_depth=10, n_estimator=100)
    
        1.GBM ì •í™•ë„: 81.5988%
        
        2.LightGBM ì •í™•ë„: 81.3995%
        
        3.XGBoost ì •í™•ë„: 79.5626%
        
        4.CatBoost ì •í™•ë„: 78.5876%
        
        5.AdaBoost ì •í™•ë„: 72.4790%
        

  
</details>


## 5. Semi-supervised Learning
<details>
    <summary> View Contents </summary>
    
  * Semi-supervised Learning
    * ì‹¤ì œ ë°ì´í„°ë“¤ì€ label dataê°€ ì ê³ , unlabeled dataê°€ ë§ì•„ì„œ ë•Œ ì‚¬ìš©ê°€ëŠ¥í•œ ê¸°ë²•ì„
    * ì´ ë•Œ labeled dataì— ëŒ€í•´ì„  supervised lossë¥¼ ì‚¬ìš©í•˜ë‚˜ unlabeled dataì— ëŒ€í•´ì„  unsupervised lossë¥¼ ì‚¬ìš©í•¨
    * ëª©í‘œëŠ” unlabeled dataë¡œ ì‚°ì¶œëœ xì™€ ê·¸ ë°ì´í„°ì˜ ë³€í˜•ëœ ê°’ì— ì˜í•´ ì‚°ì¶œëœ x_hatì˜ ì°¨ì´ê°€ ìµœì†Œí™”ë˜ëŠ” ëª¨ë¸ì„ êµ¬ì¶•í•˜ëŠ” ê²ƒì„
    * ì¤€ì§€ë„í•™ìŠµì€ í¬ê²Œ ë‘ê°€ì§€ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŒ
    
  
      1. Consistency Regularization
        * ì¼ê´€ì„± ì œì•½ ê´€ì ì—ì„œ ì ‘ê·¼í•¨
        * Unlabled dataë“¤ë¼ë¦¬ì˜ ë¶„í¬ë‚˜ ê²°ê³¼ê°’ì„ ê°€ì§€ê³  ì¼ê´€ì„±ì„ ìœ ì§€í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµí•¨


      2. Holisitic Methods
        * ì¢…í•©ì ì¸ ê´€ì ì—ì„œ ì ‘ê·¼í•¨
        * ì—¬ëŸ¬ semi-supervised learning ê¸°ë²•ë“¤ì„ í†µí•©í•˜ê³  Mixup data augmentationì„ ì ìš©í•˜ì—¬ í•™ìŠµí•¨
    


  * ì•„ë˜ Consistency Regularizationê¸°ë°˜ ê¸°ë²•ë“¤ì„ ì†Œê°œí•¨
      1. ğš·âˆ’Model

          * 2015ë…„ ì¶œì‹œëœ Ladder Networkì—ì„  Layer-wise latent vectorë“¤ì˜ consistencyë¥¼ ê³ ë ¤í•˜ì˜€ë‹¤ë©´,
          * íŒŒì´ëª¨ë¸ì—ì„ , latent vectorê°€ ì•„ë‹Œ Output vectorë“¤ì˜ consistencyë¥¼ ê³ ë ¤í•¨
          * í•˜ë‚˜ì˜ FFN(Feed-Forward Neural Network)ì— 2ë²ˆì˜ Perturbation(ë³€í˜•)ì„ ì ìš©í•¨
          * Supervised loss: Cross Entropy
          * Unsupervised loss: MSE 
          * Total loss = Cross Entropy + w*MSE
          
          
          
          ![image](https://user-images.githubusercontent.com/77199749/209635145-558c64fb-c55b-4cdc-b268-258597c9dca4.png)

          
          
      2. Temporal Ensemble

          * íŒŒì´ëª¨ë¸ì˜ í•œê³„ì ì´ â€˜single networkï¼‡ì´ì—ˆê¸° ë•Œë¬¸ì—,
          * Multiple previous network evaluationì˜ ì˜ˆì¸¡ ê°’ë“¤ì„ ì•™ìƒë¸” predictionìœ¼ë¡œ ì·¨í•©í•¨
          * Teacher ëª¨ë¸ì˜ Outputì´ ë¶ˆì•ˆì •(noisy)í•˜ë¯€ë¡œ, EMAë¡œ ëˆ„ì í•´ ì•ˆì •ì„±ì„ ë†’ì„
          * (ë‹¨ì ) Epochë§ˆë‹¤ ë°ì´í„° Zë¥¼ ë³´ê´€í•  ìš©ëŸ‰ì´ í•„ìš”í•¨ <-- ëˆ„ì ëœ ë²¡í„°ê°’ì´ Zì— ì €ì¥
          
          
          
          ![image](https://user-images.githubusercontent.com/77199749/209634856-e0eca228-ca59-40ab-b469-25c92ca99756.png)

        
          
      3. Mean Teacher

          * ìƒˆë¡œ í•™ìŠµëœ ì •ë³´ëŠ” ê° epochë‹¹ í•œ ë²ˆë§Œ ì—…ë°ì´íŠ¸ë˜ê¸° ë•Œë¬¸ì— ëŠë¦° ì†ë„ë¡œ í•™ìŠµì— ë°˜ì˜ë¨
          * íŒŒì´ëª¨ë¸ ì—ì„ , ê°™ì€ ëª¨ë¸(êµ¬ì¡°)ì´ teacherì™€ studentì˜ ì—­í• ì„ ëª¨ë‘ ê°ë‹¹í•¨ --> ì˜¤ë¶„ë¥˜ë  í™•ë¥ ì´ ë†’ìŒ
          * ë”°ë¼ì„œ, íŒŒì´ëª¨ë¸ê³¼ ë‹¤ë¥´ê²Œ targetì˜ qualityê°€ ê°œì„ ë˜ì–´ì•¼ í•¨!
          * ê°œì„  ë°©ë²•: perturbationsì„ ì‹ ì¤‘íˆ í•¨ or teacher modelì„ studentì™€ ë‹¤ë¥¸ ëª¨ë¸ì„ ì‚¬ìš©
    
    
    
          ![image](https://user-images.githubusercontent.com/77199749/209635382-08441dfa-fef6-45fb-a469-8c790bc1e830.png)

          


         
          
      4. Dual Students

          * í•™ìŠµì„ ë¬´ìˆ˜íˆ ë°˜ë³µí•˜ì˜€ì„ ë•Œ, teacher modelì€ student modelë¡œ ìˆ˜ë ´í•˜ê²Œ ë  ê²ƒì„
          * ì–´ë– í•œ biased & unstable predictionsë“¤ë„ ë‹¤ student modelë¡œ ìˆ˜í–‰ë˜ê¸° ë•Œë¬¸
          * (í•´ê²°ì±…) EMA teacher ëª¨ë¸ì´ ë‹¤ë¥¸ student ëª¨ë¸ë¡œ ëŒ€ì‹  ë˜ì–´ì•¼í•¨! Teacherë¥¼ ì—†ì• ì!!
          
     
          
          ![image](https://user-images.githubusercontent.com/77199749/209633956-0eec198e-1d7d-4bed-b2cf-3a866c5e7554.png)


          
          
      5. FastSWA(Stochastic Weight Averaging)

          * (íŒŒì´ëª¨ë¸ê³¼ mean teacherì˜ í•œê³„ì 1) ì¤‘ìš”í•œ ë‹¨ê³„ë“¤ì„ í›ˆë ¨ì´ ëë‚˜ê°ˆ ë•Œì— weight spaceì—ì„œ ë²Œì–´ì§.
          * (íŒŒì´ëª¨ë¸ê³¼ mean teacherì˜ í•œê³„ì 2) ë˜í•œ, í›ˆë ¨ì´ ëë‚˜ ê°ˆ ë•Œì¦ˆìŒ, flat regionì´ ìƒê¹€ --> í›ˆë ¨ ë§‰ë°”ì§€ì—ë„ ë‹¤ì–‘í•œ predictionsê°’ì„ ì‚°ì¶œ
          * (Resolution) Cyclic learning rateì„ ì‚¬ìš©í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ í‰ê· ëƒ„: Stochastic Weight Averaging(SWA)
          * ëª‡ë²ˆì˜ epochì´ ì§€ë‚˜ë©´, learning rateë¥¼ ë°”ê¾¸ì–´ì„œ í•™ìŠµì„ several cyclesë™ì•ˆ ë°˜ë³µí•¨
          * (SWA) Cycleì˜ ë§ˆì§€ë§‰ weightê°’(=learning rateì˜ ìµœì†Œê°’)ì„ ì €ì¥í•˜ê³  í‰ê· ë‚´ì„œ ì‚¬ìš©í•¨
          * (Fast-SWA) í•œ Cycleë‚´ ì—¬ëŸ¬ ê°œì˜ weightê°’ë“¤ì„ ì €ì¥í•˜ê³  í‰ê· ë‚´ì„œ ì‚¬ìš©í•¨![image](https://user-images.githubusercontent.com/77199749/209634374-ff0c971d-1ff6-4235-97c7-76e6bfa01e8d.png)



          
          ![image](https://user-images.githubusercontent.com/77199749/209634228-c69b5103-6399-49a1-aece-78523206636f.png)
    
    
    
          ![image](https://user-images.githubusercontent.com/77199749/209634711-d2db1e60-d193-4dd7-9073-166dd9de7c6b.png)
        

          
          
       6. Virtual Adversarial Training(VAT)

          * ì ëŒ€ì  í•™ìŠµ(Adversarial training)ê¸°ë²•ì„ í™œìš©í•´ ëª¨ë¸ì´ ê°€ì¥ ì·¨ì•½í•œ ë°©í–¥ìœ¼ë¡œ í•™ìŠµ
          * ëª¨ë¸ì˜ ê°•ê±´ì„±ì„ ë†’ì„
          * ì›ë³¸ì´ë¯¸ì§€ì™€ ì ëŒ€ì í•™ìŠµ ì´ë¯¸ì§€ì˜ lossê°’ì„ í†µí•˜ì—¬ í•™ìŠµí•¨
    
    
          ![image](https://user-images.githubusercontent.com/77199749/209635561-402803a8-e0dc-4c30-aa40-ac44b4bc38da.png)
    
    
    
          ![image](https://user-images.githubusercontent.com/77199749/209635597-cc08cddc-4f6f-47f8-a736-181f69984c91.png)


          
          
          

   


    * ìµœì¢…ì‹¤í—˜ê²°ê³¼ë¹„êµ(ì •í™•ë„ ë†’ì€ìˆœ, dataset: CIFAR10, batch_size = 256)
    
        1.VAT ì •í™•ë„: 65.07% (0.597 iter/sec)
        
        2.Mean Teacher ì •í™•ë„: 59.29% (0.759 iter/sec)
        
        3.Pi-Model ì •í™•ë„: 59.14% (0.886 iter/sec)
        
        ** ê²°ê³¼í•´ì„:
        ë¨¼ì €, CIFAR-10 ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ ë™ì¼í•œ íŒŒë¼ë¯¸í„°ë¡œ ì‹¤í—˜
    
        
        3ê°€ì§€ ëª¨ë¸ì˜ trainable parameterëŠ” 1467610ë¡œ ê³ ì •í•˜ì˜€ìœ¼ë¯€ë¡œ, ì‘ì€ ë…¸ì´ì¦ˆì— ì·¨ì•½í•˜ì§€ ì•Šì€ ê°•ê±´í•œ ëª¨ë¸ì¸ VATì˜ ì„±ëŠ¥ì´ ê°€ì¥ ë†’ì€ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. teacherì™€ studentë¥¼ ë¶„ë¦¬í•˜ì—¬ í•™ìŠµí•œ mean teacherëŠ” ì†ë„ì™€ ì„±ëŠ¥ ë©´ì—ì„œ pi-modelì— ë¹„í•´ ì¦ê°€í•˜ì˜€ìœ¼ë‚˜, VATì²˜ëŸ¼ í° ë³€í™”ëŠ” ì—†ì—ˆë‹¤. ì¼ê´€ì„± ì œì•½ì˜ ì ‘ê·¼ì„ ê³ ë ¤í•˜ì˜€ì„ ë•Œ, ì´ë¯¸ì§€ë“¤ì˜ ë¶„ë¥˜ì„±ëŠ¥ì„ ê°€ì¥ ë†’ì¼ ìˆ˜ ìˆëŠ” ì¤€ì§€ë„ í•™ìŠµ ëª¨ë¸ì€ VATì¸ ê²ƒì„ ì†ë„ì™€ ì„±ëŠ¥ë©´ì—ì„œ ëª¨ë‘ í™•ì¸ í•  ìˆ˜ ìˆì—ˆë‹¤.
        
        
        

    * ìµœì¢…ì‹¤í—˜ê²°ê³¼ë¹„êµ(ì •í™•ë„ ë†’ì€ìˆœ, dataset: MNIST, batch_size = 64)
    
        1.Mean Teacher ì •í™•ë„: 99.38%
        
        2.Temporal Ensemble ì •í™•ë„: 95.20%
        
        ** ì „ì²´ ê²°ê³¼í•´ì„:
        ë¨¼ì €, MNIST ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ ë™ì¼í•œ íŒŒë¼ë¯¸í„°ë¡œ ì‹¤í—˜    
    
        
        Temporal Ensembleì—ì„  outputì´ ë¶ˆì•ˆì •í•˜ì—¬ EMA(Exponential moving average)ë¡œ ëˆ„ì í•˜ì—¬ ì•ˆì •ì„±ì„ ë†’ì¸ ê²ƒì„ íƒí•˜ì˜€ì§€ë§Œ, mean teacherì—ì„œëŠ” teacherì™€ studentë¥¼ ê°ê° ì§€ì •í•´ 'studentì˜ ê°€ì¤‘ì¹˜ë¥¼ EMAí•˜ì—¬ teacherì— ì‚¬ìš©'í•˜ì˜€ë‹¤. ê²°ê³¼ì—ì„œë„ ë³¼ ìˆ˜ ìˆë“¯ì´, Temporal Ensemblingì˜ ì£¼ìš” ê¸°ë²•ì¸ outputì˜ í‰ê· ê°’ì„ ì ìš©í•˜ëŠ” ê²ƒë³´ë‹¤, Mean teacherì²˜ëŸ¼, teacherì™€ studentë¥¼ ì§€ì •í•˜ì—¬ì„œ í•™ìŠµí•˜ê²Œ í•˜ëŠ” ê²ƒì´ ë©”ëª¨ë¦¬ì˜ ë¶€ë‹´ë„ ì ê³  ì†ë„ì™€ ì„±ëŠ¥ë©´ì—ì„œ ë›°ì–´ë‚œ ê²ƒì„ ì•Œ ìˆ˜ ìˆì—ˆë‹¤.
        

  
</details>
    
=========================================================================
## Reference
- https://sustaining-starflower-aff.notion.site/2022-2-0e068bff3023401fa9fa13e96c0269d7 (ê°•í•„ì„± êµìˆ˜ë‹˜ baìˆ˜ì—…ìë£Œ)
