{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfbc6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "*Semi-Supervised Learning: Supervised-learning(지도학습)과 Unsupervised-learning(비지도학습)의 혼합된 방식임\n",
    "- 소수의 labeled data가 존재하고, 다수의 unlabeled data가 존재할 때 사용함\n",
    "- 이때, labeled data에 대해선 supervised loss를 사용하나\n",
    "- unlabeled data에 대해선 unsuperivesed loss를 사용함 \n",
    "--> unlabeled data와 그 데이터의 변형된 값에 의해 산출된 output들의 차이가 작은 모델 구축\n",
    "- 두가지 대표적인 기법 존재: Consistency Regularization(일관성 관점), Holistic Methods(종합적인 관점)\n",
    "\n",
    "<Consistency Regularization 계열 모델>\n",
    "1. Pie-Model(2016)\n",
    "2. Temporal Ensemble(2016)\n",
    "3. Virtual Adversarial Training(2017)\n",
    "4. Mean Teacher(2017)\n",
    "\n",
    "*위 4가지의 모델을 동일한 조건에서, 두 개의 데이터 셋을 놓고 5번의 비교실험을 진행함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e385828",
   "metadata": {},
   "source": [
    "## Dataset: CIFAR-10 and MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6183243",
   "metadata": {},
   "outputs": [],
   "source": [
    "*데이터 셋은 CIFAR-10과 MNIST데이터를 사용함\n",
    "*CIFAR-10: CLASS수는 총 10가지인 (3,32,32)차원의 '컬러'이미지 데이터임.\n",
    "    60000개(학습용 50000, 테스트용 10000)의 데이터셋으로 구성됨\n",
    "*MNIST: 0~9까지의 숫자를 손글씨로 적은 (1,28,28)차원의 '흑백'이미지 데이터임(0~9까지의 손글씨이므로 CLASS수도 당연히 10개임),\n",
    "    70000개(학습용 60000, 테스트용 10000)의 데이터셋으로 구성됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93395868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73176e29",
   "metadata": {},
   "source": [
    "## CIFAR-10 데이터셋을 활용하여 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b536eaed",
   "metadata": {},
   "source": [
    "## Pi-Model(2016) vs Mean Teacher(2017) vs VAT(2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2285a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "*Pi-Model(2016):\n",
    "    2015년 출시된 Ladder Network에선 Layer-wise latent vector들의 consistency를 고려하였다면,\n",
    "    파이모델에선, latent vector가 아닌 Output vector들의 consistency를 고려함\n",
    "    \n",
    "    하나의 FFN(Feed-Forward Neural Network)에 2번의 Perturbation(변형)을 적용함\n",
    "    - Supervised loss: Cross Entropy\n",
    "    - Unsupervised loss: MSE \n",
    "    - Total loss = Cross Entropy + w*MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7a0bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "* Mean Teacher(2017): \n",
    "    새로 학습된 정보는 각 epoch당 한 번만 업데이트되기 때문에 느린 속도로 학습에 반영됨\n",
    "    \n",
    "    파이모델 에선, 같은 모델(구조)이 teacher와 student의 역할을 모두 감당함\n",
    "    --> 오분류될 확률이 높음\n",
    "    따라서, 파이모델과 다르게 target의 quality가 개선되어야 함!\n",
    "    \n",
    "    -개선 방법: perturbations을 신중히 함 or teacher model을 student와 다른 모델을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9d3be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "* Virtual Adversarial Training(2017):\n",
    "    적대적 학습(Adversarial training)기법을 활용해 모델이 가장 취약한 방향으로 학습\n",
    "    --> 모델의 강건성을 높임\n",
    "    \n",
    "    원본이미지와 적대적학습 이미지의 loss값을 통하여 학습함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd076c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "*출처: https://github.com/perrying/realistic-ssl-evaluation-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5186d7",
   "metadata": {},
   "source": [
    "#### dataset 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "046edcf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\coursework\\\\ba\\\\2022_BA_donghwanshin\\\\05. Semi-supervised Learning'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51154337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ppuya\\anaconda3\\envs\\jff\\lib\\site-packages\\ipykernel_launcher.py:68: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "import argparse, os\n",
    "import numpy as np\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--seed\", \"-s\", default=1, type=int, help=\"random seed\")\n",
    "parser.add_argument(\"--dataset\", \"-d\", default=\"cifar10\", type=str, help=\"dataset name : [svhn, cifar10]\")\n",
    "parser.add_argument(\"--nlabels\", \"-n\", default=1000, type=int, help=\"the number of labeled data\")\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "COUNTS = {\n",
    "    \"svhn\": {\"train\": 73257, \"test\": 26032, \"valid\": 7326, \"extra\": 531131},\n",
    "    \"cifar10\": {\"train\": 50000, \"test\": 10000, \"valid\": 5000, \"extra\": 0},\n",
    "    \"imagenet_32\": {\n",
    "        \"train\": 1281167,\n",
    "        \"test\": 50000,\n",
    "        \"valid\": 50050,\n",
    "        \"extra\": 0,\n",
    "    },\n",
    "}\n",
    "\n",
    "_DATA_DIR = \"./data\"\n",
    "\n",
    "def split_l_u(train_set, n_labels):\n",
    "    # NOTE: this function assume that train_set is shuffled.\n",
    "    images = train_set[\"images\"]\n",
    "    labels = train_set[\"labels\"]\n",
    "    classes = np.unique(labels)\n",
    "    n_labels_per_cls = n_labels // len(classes)\n",
    "    l_images = []\n",
    "    l_labels = []\n",
    "    u_images = []\n",
    "    u_labels = []\n",
    "    for c in classes:\n",
    "        cls_mask = (labels == c)\n",
    "        c_images = images[cls_mask]\n",
    "        c_labels = labels[cls_mask]\n",
    "        l_images += [c_images[:n_labels_per_cls]]\n",
    "        l_labels += [c_labels[:n_labels_per_cls]]\n",
    "        u_images += [c_images[n_labels_per_cls:]]\n",
    "        u_labels += [np.zeros_like(c_labels[n_labels_per_cls:]) - 1] # dammy label\n",
    "    l_train_set = {\"images\": np.concatenate(l_images, 0), \"labels\": np.concatenate(l_labels, 0)}\n",
    "    u_train_set = {\"images\": np.concatenate(u_images, 0), \"labels\": np.concatenate(u_labels, 0)}\n",
    "    return l_train_set, u_train_set\n",
    "\n",
    "def _load_svhn():\n",
    "    splits = {}\n",
    "    for split in [\"train\", \"test\", \"extra\"]:\n",
    "        tv_data = datasets.SVHN(_DATA_DIR, split, download=True)\n",
    "        data = {}\n",
    "        data[\"images\"] = tv_data.data\n",
    "        data[\"labels\"] = tv_data.labels\n",
    "        splits[split] = data\n",
    "    return splits.values()\n",
    "\n",
    "def _load_cifar10():\n",
    "    splits = {}\n",
    "    for train in [True, False]:\n",
    "        tv_data = datasets.CIFAR10(_DATA_DIR, train, download=True)\n",
    "        data = {}\n",
    "        data[\"images\"] = tv_data.data\n",
    "        data[\"labels\"] = np.array(tv_data.targets)\n",
    "        splits[\"train\" if train else \"test\"] = data\n",
    "    return splits.values()\n",
    "\n",
    "def gcn(images, multiplier=55, eps=1e-10):\n",
    "    # global contrast normalization\n",
    "    images = images.astype(np.float)\n",
    "    images -= images.mean(axis=(1,2,3), keepdims=True)\n",
    "    per_image_norm = np.sqrt(np.square(images).sum((1,2,3), keepdims=True))\n",
    "    per_image_norm[per_image_norm < eps] = 1\n",
    "    return multiplier * images / per_image_norm\n",
    "\n",
    "def get_zca_normalization_param(images, scale=0.1, eps=1e-10):\n",
    "    n_data, height, width, channels = images.shape\n",
    "    images = images.reshape(n_data, height*width*channels)\n",
    "    image_cov = np.cov(images, rowvar=False)\n",
    "    U, S, _ = np.linalg.svd(image_cov + scale * np.eye(image_cov.shape[0]))\n",
    "    zca_decomp = np.dot(U, np.dot(np.diag(1/np.sqrt(S + eps)), U.T))\n",
    "    mean = images.mean(axis=0)\n",
    "    return mean, zca_decomp\n",
    "\n",
    "def zca_normalization(images, mean, decomp):\n",
    "    n_data, height, width, channels = images.shape\n",
    "    images = images.reshape(n_data, -1)\n",
    "    images = np.dot((images - mean), decomp)\n",
    "    return images.reshape(n_data, height, width, channels)\n",
    "\n",
    "rng = np.random.RandomState(args.seed)\n",
    "\n",
    "validation_count = COUNTS[args.dataset][\"valid\"]\n",
    "\n",
    "extra_set = None  # In general, there won't be extra data.\n",
    "if args.dataset == \"svhn\":\n",
    "    train_set, test_set, extra_set = _load_svhn()\n",
    "elif args.dataset == \"cifar10\":\n",
    "    train_set, test_set = _load_cifar10()\n",
    "    train_set[\"images\"] = gcn(train_set[\"images\"])\n",
    "    test_set[\"images\"] = gcn(test_set[\"images\"])\n",
    "    mean, zca_decomp = get_zca_normalization_param(train_set[\"images\"])\n",
    "    train_set[\"images\"] = zca_normalization(train_set[\"images\"], mean, zca_decomp)\n",
    "    test_set[\"images\"] = zca_normalization(test_set[\"images\"], mean, zca_decomp)\n",
    "    # N x H x W x C -> N x C x H x W\n",
    "    train_set[\"images\"] = np.transpose(train_set[\"images\"], (0,3,1,2))\n",
    "    test_set[\"images\"] = np.transpose(test_set[\"images\"], (0,3,1,2))\n",
    "\n",
    "# permute index of training set\n",
    "indices = rng.permutation(len(train_set[\"images\"]))\n",
    "train_set[\"images\"] = train_set[\"images\"][indices]\n",
    "train_set[\"labels\"] = train_set[\"labels\"][indices]\n",
    "\n",
    "if extra_set is not None:\n",
    "    extra_indices = rng.permutation(len(extra_set[\"images\"]))\n",
    "    extra_set[\"images\"] = extra_set[\"images\"][extra_indices]\n",
    "    extra_set[\"labels\"] = extra_set[\"labels\"][extra_indices]\n",
    "\n",
    "# split training set into training and validation\n",
    "train_images = train_set[\"images\"][validation_count:]\n",
    "train_labels = train_set[\"labels\"][validation_count:]\n",
    "validation_images = train_set[\"images\"][:validation_count]\n",
    "validation_labels = train_set[\"labels\"][:validation_count]\n",
    "validation_set = {\"images\": validation_images, \"labels\": validation_labels}\n",
    "train_set = {\"images\": train_images, \"labels\": train_labels}\n",
    "\n",
    "# split training set into labeled data and unlabeled data\n",
    "l_train_set, u_train_set = split_l_u(train_set, args.nlabels)\n",
    "\n",
    "if not os.path.exists(os.path.join(_DATA_DIR, args.dataset)):\n",
    "    os.mkdir(os.path.join(_DATA_DIR, args.dataset))\n",
    "\n",
    "np.save(os.path.join(_DATA_DIR, args.dataset, \"l_train\"), l_train_set)\n",
    "np.save(os.path.join(_DATA_DIR, args.dataset, \"u_train\"), u_train_set)\n",
    "np.save(os.path.join(_DATA_DIR, args.dataset, \"val\"), validation_set)\n",
    "np.save(os.path.join(_DATA_DIR, args.dataset, \"test\"), test_set)\n",
    "if extra_set is not None:\n",
    "    np.save(os.path.join(_DATA_DIR, args.dataset, \"extra\"), extra_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6fad2f",
   "metadata": {},
   "source": [
    "#### 데이터 및 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1b01d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import argparse, math, time, json, os\n",
    "\n",
    "import wrn, transform\n",
    "from config import config\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--alg\", \"-a\", default=\"PI\", type=str, help=\"ssl algorithm : [supervised, PI, MT, VAT, PL, ICT]\")\n",
    "parser.add_argument(\"--em\", default=0, type=float, help=\"coefficient of entropy minimization. If you try VAT + EM, set 0.06\")\n",
    "parser.add_argument(\"--validation\", default=25000, type=int, help=\"validate at this interval (default 25000)\")\n",
    "parser.add_argument(\"--dataset\", \"-d\", default=\"svhn\", type=str, help=\"dataset name : [svhn, cifar10]\")\n",
    "parser.add_argument(\"--root\", \"-r\", default=\"data\", type=str, help=\"dataset dir\")\n",
    "parser.add_argument(\"--output\", \"-o\", default=\"./exp_res\", type=str, help=\"output dir\")\n",
    "args, _ = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "268057d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holizontal flip : True, random crop : True, gaussian noise : True\n",
      "labeled data : 1000, unlabeled data : 44000, training data : 45000\n",
      "validation data : 5000, test data : 10000\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "condition = {}\n",
    "exp_name = \"\"\n",
    "args.dataset = 'cifar10'\n",
    "condition[\"dataset\"] = args.dataset\n",
    "exp_name += str(args.dataset) + \"_\"\n",
    "dataset_cfg = config[args.dataset]\n",
    "transform_fn = transform.transform(*dataset_cfg[\"transform\"]) # transform function (flip, crop, noise)\n",
    "\n",
    "\n",
    "l_train_dataset = dataset_cfg[\"dataset\"](args.root, \"l_train\")\n",
    "u_train_dataset = dataset_cfg[\"dataset\"](args.root, \"u_train\")\n",
    "val_dataset = dataset_cfg[\"dataset\"](args.root, \"val\")\n",
    "test_dataset = dataset_cfg[\"dataset\"](args.root, \"test\")\n",
    "\n",
    "print(\"labeled data : {}, unlabeled data : {}, training data : {}\".format(\n",
    "    len(l_train_dataset), len(u_train_dataset), len(l_train_dataset)+len(u_train_dataset)))\n",
    "print(\"validation data : {}, test data : {}\".format(len(val_dataset), len(test_dataset)))\n",
    "condition[\"number_of_data\"] = {\n",
    "    \"labeled\":len(l_train_dataset), \"unlabeled\":len(u_train_dataset),\n",
    "    \"validation\":len(val_dataset), \"test\":len(test_dataset)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62bf74ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'iteration': 8800,\n",
       " 'warmup': 1000,\n",
       " 'lr_decay_iter': 400,\n",
       " 'lr_decay_factor': 0.2,\n",
       " 'batch_size': 256}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config[\"shared\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25c848cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomSampler(torch.utils.data.Sampler):\n",
    "    \"\"\" sampling without replacement \"\"\"\n",
    "    def __init__(self, num_data, num_sample):\n",
    "        iterations = num_sample // num_data + 1\n",
    "        self.indices = torch.cat([torch.randperm(num_data) for _ in range(iterations)]).tolist()[:num_sample]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "shared_cfg = config[\"shared\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93a55dd",
   "metadata": {},
   "source": [
    "### Pi-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ad4ca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.alg = 'PI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0032620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algorithm : PI\n",
      "maximum iteration : 8800\n",
      "parameters :  {'lr': 0.0003, 'consis_coef': 20.0}\n",
      "trainable parameters : 1467610\n"
     ]
    }
   ],
   "source": [
    "if args.alg != \"supervised\":\n",
    "    # batch size = 0.5 x batch size\n",
    "    l_loader = DataLoader(\n",
    "        l_train_dataset, shared_cfg[\"batch_size\"]//2, drop_last=True,\n",
    "        sampler=RandomSampler(len(l_train_dataset), shared_cfg[\"iteration\"] * shared_cfg[\"batch_size\"]//2)\n",
    "    )\n",
    "else:\n",
    "    l_loader = DataLoader(\n",
    "        l_train_dataset, shared_cfg[\"batch_size\"], drop_last=True,\n",
    "        sampler=RandomSampler(len(l_train_dataset), shared_cfg[\"iteration\"] * shared_cfg[\"batch_size\"])\n",
    "    )\n",
    "print(\"algorithm : {}\".format(args.alg))\n",
    "condition[\"algorithm\"] = args.alg\n",
    "exp_name += str(args.alg) + \"_\"\n",
    "\n",
    "u_loader = DataLoader(\n",
    "    u_train_dataset, shared_cfg[\"batch_size\"]//2, drop_last=True,\n",
    "    sampler=RandomSampler(len(u_train_dataset), shared_cfg[\"iteration\"] * shared_cfg[\"batch_size\"]//2)\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(val_dataset, 128, shuffle=False, drop_last=False)\n",
    "test_loader = DataLoader(test_dataset, 128, shuffle=False, drop_last=False)\n",
    "\n",
    "print(\"maximum iteration : {}\".format(min(len(l_loader), len(u_loader))))\n",
    "\n",
    "alg_cfg = config[args.alg]\n",
    "print(\"parameters : \", alg_cfg)\n",
    "condition[\"h_parameters\"] = alg_cfg\n",
    "\n",
    "if args.em > 0:\n",
    "    print(\"entropy minimization : {}\".format(args.em))\n",
    "    exp_name += \"em_\"\n",
    "condition[\"entropy_maximization\"] = args.em\n",
    "\n",
    "model = wrn.WRN(2, dataset_cfg[\"num_classes\"], transform_fn).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=alg_cfg[\"lr\"])\n",
    "\n",
    "trainable_paramters = sum([p.data.nelement() for p in model.parameters()])\n",
    "print(\"trainable parameters : {}\".format(trainable_paramters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb4635ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration [1/8800] cls loss : 1.193980e+00, SSL loss : 1.643282e-05, coef : 1.36113e-01, time : 73.715 iter/sec, rest : 1.989 min, lr : 0.0003 \n",
      "iteration [1000/8800] cls loss : 6.385348e-02, SSL loss : 1.389795e-01, coef : 2.00000e+01, time : 0.890 iter/sec, rest : 146.143 min, lr : 5.9999999999999995e-05 \n",
      "iteration [2000/8800] cls loss : 3.971205e-02, SSL loss : 1.439586e-01, coef : 2.00000e+01, time : 0.885 iter/sec, rest : 127.999 min, lr : 5.9999999999999995e-05 \n",
      "iteration [3000/8800] cls loss : 2.138251e-02, SSL loss : 1.379075e-01, coef : 2.00000e+01, time : 0.886 iter/sec, rest : 109.090 min, lr : 5.9999999999999995e-05 \n",
      "iteration [4000/8800] cls loss : 2.173064e-02, SSL loss : 1.026384e-01, coef : 2.00000e+01, time : 0.885 iter/sec, rest : 90.377 min, lr : 5.9999999999999995e-05 \n",
      "iteration [5000/8800] cls loss : 1.885763e-02, SSL loss : 5.245686e-02, coef : 2.00000e+01, time : 0.884 iter/sec, rest : 71.618 min, lr : 5.9999999999999995e-05 \n",
      "iteration [6000/8800] cls loss : 2.210621e-02, SSL loss : 7.996042e-02, coef : 2.00000e+01, time : 0.885 iter/sec, rest : 52.757 min, lr : 5.9999999999999995e-05 \n",
      "iteration [7000/8800] cls loss : 2.002053e-02, SSL loss : 6.686549e-02, coef : 2.00000e+01, time : 0.885 iter/sec, rest : 33.906 min, lr : 5.9999999999999995e-05 \n",
      "iteration [8000/8800] cls loss : 2.739096e-02, SSL loss : 8.114515e-02, coef : 2.00000e+01, time : 0.886 iter/sec, rest : 15.055 min, lr : 5.9999999999999995e-05 \n",
      "\n",
      "### validation ###\n",
      "[40/40] time : 112.4 data/sec, rest : 0.00 sec \n",
      "varidation accuracy : 0.5963999629020691\n",
      "### test ###\n",
      "[70/79] time : 1148.4 data/sec, rest : 0.01 sec \n",
      "test accuracy : 0.5913999676704407\n",
      "test acc : 0.5913999676704407\n"
     ]
    }
   ],
   "source": [
    "if args.alg == \"VAT\": # virtual adversarial training\n",
    "    from vat import VAT\n",
    "    ssl_obj = VAT(alg_cfg[\"eps\"][args.dataset], alg_cfg[\"xi\"], 1)\n",
    "elif args.alg == \"MT\": # mean teacher\n",
    "    from mean_teacher import MT\n",
    "    t_model = wrn.WRN(2, dataset_cfg[\"num_classes\"], transform_fn).to(device)\n",
    "    t_model.load_state_dict(model.state_dict())\n",
    "    ssl_obj = MT(t_model, alg_cfg[\"ema_factor\"])\n",
    "elif args.alg == \"PI\": # PI Model\n",
    "    from pimodel import PiModel\n",
    "    ssl_obj = PiModel()\n",
    "else:\n",
    "    raise ValueError(\"{} is unknown algorithm\".format(args.alg))\n",
    "\n",
    "print()\n",
    "iteration = 0\n",
    "maximum_val_acc = 0\n",
    "s = time.time()\n",
    "for l_data, u_data in zip(l_loader, u_loader):\n",
    "    iteration += 1\n",
    "    l_input, target = l_data\n",
    "    l_input, target = l_input.to(device).float(), target.to(device).long()\n",
    "\n",
    "    if args.alg != \"supervised\": # for ssl algorithm\n",
    "        u_input, dummy_target = u_data\n",
    "        u_input, dummy_target = u_input.to(device).float(), dummy_target.to(device).long()\n",
    "\n",
    "        target = torch.cat([target, dummy_target], 0)\n",
    "        unlabeled_mask = (target == -1).float()\n",
    "\n",
    "        inputs = torch.cat([l_input, u_input], 0)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # ramp up exp(-5(1 - t)^2)\n",
    "        coef = alg_cfg[\"consis_coef\"] * math.exp(-5 * (1 - min(iteration/shared_cfg[\"warmup\"], 1))**2)\n",
    "        ssl_loss = ssl_obj(inputs, outputs.detach(), model, unlabeled_mask) * coef\n",
    "\n",
    "    else:\n",
    "        outputs = model(l_input)\n",
    "        coef = 0\n",
    "        ssl_loss = torch.zeros(1).to(device)\n",
    "\n",
    "    # supervised loss\n",
    "    cls_loss = F.cross_entropy(outputs, target, reduction=\"none\", ignore_index=-1).mean()\n",
    "\n",
    "    loss = cls_loss + ssl_loss\n",
    "\n",
    "    if args.em > 0:\n",
    "        loss -= args.em * ((outputs.softmax(1) * F.log_softmax(outputs, 1)).sum(1) * unlabeled_mask).mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if args.alg == \"MT\" or args.alg == \"ICT\":\n",
    "        # parameter update with exponential moving average\n",
    "        ssl_obj.moving_average(model.parameters())\n",
    "    # display\n",
    "    if iteration == 1 or (iteration % 1000) == 0:\n",
    "        wasted_time = time.time() - s\n",
    "        rest = (shared_cfg[\"iteration\"] - iteration)/100 * wasted_time / 60\n",
    "        print(\"iteration [{}/{}] cls loss : {:.6e}, SSL loss : {:.6e}, coef : {:.5e}, time : {:.3f} iter/sec, rest : {:.3f} min, lr : {}\".format(\n",
    "            iteration, shared_cfg[\"iteration\"], cls_loss.item(), ssl_loss.item(), coef, 100 / wasted_time, rest, optimizer.param_groups[0][\"lr\"]),\n",
    "            \"\\r\")\n",
    "        s = time.time()\n",
    "\n",
    "    # validation\n",
    "    if (iteration % args.validation) == 0 or iteration == shared_cfg[\"iteration\"]:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            print()\n",
    "            print(\"### validation ###\")\n",
    "            sum_acc = 0.\n",
    "            s = time.time()\n",
    "            for j, data in enumerate(val_loader):\n",
    "                input, target = data\n",
    "                input, target = input.to(device).float(), target.to(device).long()\n",
    "\n",
    "                output = model(input)\n",
    "\n",
    "                pred_label = output.max(1)[1]\n",
    "                sum_acc += (pred_label == target).float().sum()\n",
    "                if ((j+1) % 10) == 0:\n",
    "                    d_p_s = 10/(time.time()-s)\n",
    "                    print(\"[{}/{}] time : {:.1f} data/sec, rest : {:.2f} sec\".format(\n",
    "                        j+1, len(val_loader), d_p_s, (len(val_loader) - j-1)/d_p_s\n",
    "                    ), \"\\r\", end=\"\")\n",
    "                    s = time.time()\n",
    "            acc = sum_acc/float(len(val_dataset))\n",
    "            print()\n",
    "            print(\"varidation accuracy : {}\".format(acc))\n",
    "            # test\n",
    "            if maximum_val_acc < acc:\n",
    "                print(\"### test ###\")\n",
    "                maximum_val_acc = acc\n",
    "                sum_acc = 0.\n",
    "                s = time.time()\n",
    "                for j, data in enumerate(test_loader):\n",
    "                    input, target = data\n",
    "                    input, target = input.to(device).float(), target.to(device).long()\n",
    "                    output = model(input)\n",
    "                    pred_label = output.max(1)[1]\n",
    "                    sum_acc += (pred_label == target).float().sum()\n",
    "                    if ((j+1) % 10) == 0:\n",
    "                        d_p_s = 100/(time.time()-s)\n",
    "                        print(\"[{}/{}] time : {:.1f} data/sec, rest : {:.2f} sec\".format(\n",
    "                            j+1, len(test_loader), d_p_s, (len(test_loader) - j-1)/d_p_s\n",
    "                        ), \"\\r\", end=\"\")\n",
    "                        s = time.time()\n",
    "                print()\n",
    "                test_acc = sum_acc / float(len(test_dataset))\n",
    "                print(\"test accuracy : {}\".format(test_acc))\n",
    "                # torch.save(model.state_dict(), os.path.join(args.output, \"best_model.pth\"))\n",
    "        model.train()\n",
    "        s = time.time()\n",
    "    # lr decay\n",
    "    if iteration == shared_cfg[\"lr_decay_iter\"]:\n",
    "        optimizer.param_groups[0][\"lr\"] *= shared_cfg[\"lr_decay_factor\"]\n",
    "\n",
    "print(\"test acc : {}\".format(test_acc))\n",
    "condition[\"test_acc\"] = test_acc.item()\n",
    "\n",
    "exp_name += str(int(time.time())) # unique ID\n",
    "if not os.path.exists(args.output):\n",
    "    os.mkdir(args.output)\n",
    "with open(os.path.join(args.output, exp_name + \".json\"), \"w\") as f:\n",
    "    json.dump(condition, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5744156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "* 결과해석:\n",
    "    파이 모델을 CIFAR-10의 50000개의 데이터를 256의 배치 사이즈로 8800 iteration 훈련시킨 결과 \n",
    "    \"supervised loss : 0.02\", \"unsupervised loss: 0.08\"로 수렴함\n",
    "    \n",
    "    테스트 결과는 0.5914임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9378af83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f76c8ce0",
   "metadata": {},
   "source": [
    "### Mean Teacher(MT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27ebe2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.alg = 'MT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e663306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algorithm : MT\n",
      "maximum iteration : 8800\n",
      "parameters :  {'ema_factor': 0.95, 'lr': 0.0004, 'consis_coef': 8}\n",
      "trainable parameters : 1467610\n"
     ]
    }
   ],
   "source": [
    "if args.alg != \"supervised\":\n",
    "    # batch size = 0.5 x batch size\n",
    "    l_loader = DataLoader(\n",
    "        l_train_dataset, shared_cfg[\"batch_size\"]//2, drop_last=True,\n",
    "        sampler=RandomSampler(len(l_train_dataset), shared_cfg[\"iteration\"] * shared_cfg[\"batch_size\"]//2)\n",
    "    )\n",
    "else:\n",
    "    l_loader = DataLoader(\n",
    "        l_train_dataset, shared_cfg[\"batch_size\"], drop_last=True,\n",
    "        sampler=RandomSampler(len(l_train_dataset), shared_cfg[\"iteration\"] * shared_cfg[\"batch_size\"])\n",
    "    )\n",
    "print(\"algorithm : {}\".format(args.alg))\n",
    "condition[\"algorithm\"] = args.alg\n",
    "exp_name += str(args.alg) + \"_\"\n",
    "\n",
    "u_loader = DataLoader(\n",
    "    u_train_dataset, shared_cfg[\"batch_size\"]//2, drop_last=True,\n",
    "    sampler=RandomSampler(len(u_train_dataset), shared_cfg[\"iteration\"] * shared_cfg[\"batch_size\"]//2)\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(val_dataset, 128, shuffle=False, drop_last=False)\n",
    "test_loader = DataLoader(test_dataset, 128, shuffle=False, drop_last=False)\n",
    "\n",
    "print(\"maximum iteration : {}\".format(min(len(l_loader), len(u_loader))))\n",
    "\n",
    "alg_cfg = config[args.alg]\n",
    "print(\"parameters : \", alg_cfg)\n",
    "condition[\"h_parameters\"] = alg_cfg\n",
    "\n",
    "if args.em > 0:\n",
    "    print(\"entropy minimization : {}\".format(args.em))\n",
    "    exp_name += \"em_\"\n",
    "condition[\"entropy_maximization\"] = args.em\n",
    "\n",
    "model = wrn.WRN(2, dataset_cfg[\"num_classes\"], transform_fn).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=alg_cfg[\"lr\"])\n",
    "\n",
    "trainable_paramters = sum([p.data.nelement() for p in model.parameters()])\n",
    "print(\"trainable parameters : {}\".format(trainable_paramters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdbbf8c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration [1/8800] cls loss : 1.194580e+00, SSL loss : 6.223509e-06, coef : 5.44450e-02, time : 724.812 iter/sec, rest : 0.202 min, lr : 0.0004 \n",
      "iteration [1000/8800] cls loss : 4.593794e-02, SSL loss : 5.544227e-02, coef : 8.00000e+00, time : 0.759 iter/sec, rest : 171.335 min, lr : 8e-05 \n",
      "iteration [2000/8800] cls loss : 1.836864e-02, SSL loss : 4.573943e-02, coef : 8.00000e+00, time : 0.758 iter/sec, rest : 149.500 min, lr : 8e-05 \n",
      "iteration [3000/8800] cls loss : 1.258721e-02, SSL loss : 5.397944e-02, coef : 8.00000e+00, time : 0.758 iter/sec, rest : 127.467 min, lr : 8e-05 \n",
      "iteration [4000/8800] cls loss : 9.231715e-03, SSL loss : 5.635411e-02, coef : 8.00000e+00, time : 0.758 iter/sec, rest : 105.551 min, lr : 8e-05 \n",
      "iteration [5000/8800] cls loss : 6.741161e-03, SSL loss : 5.954162e-02, coef : 8.00000e+00, time : 0.758 iter/sec, rest : 83.559 min, lr : 8e-05 \n",
      "iteration [6000/8800] cls loss : 5.356046e-03, SSL loss : 2.431375e-02, coef : 8.00000e+00, time : 0.760 iter/sec, rest : 61.426 min, lr : 8e-05 \n",
      "iteration [7000/8800] cls loss : 4.826032e-03, SSL loss : 7.909431e-02, coef : 8.00000e+00, time : 0.758 iter/sec, rest : 39.589 min, lr : 8e-05 \n",
      "iteration [8000/8800] cls loss : 5.132055e-03, SSL loss : 2.340172e-02, coef : 8.00000e+00, time : 0.759 iter/sec, rest : 17.563 min, lr : 8e-05 \n",
      "\n",
      "### validation ###\n",
      "[40/40] time : 118.0 data/sec, rest : 0.00 sec \n",
      "varidation accuracy : 0.6055999994277954\n",
      "### test ###\n",
      "[70/79] time : 1157.8 data/sec, rest : 0.01 sec \n",
      "test accuracy : 0.5928999781608582\n",
      "test acc : 0.5928999781608582\n"
     ]
    }
   ],
   "source": [
    "if args.alg == \"VAT\": # virtual adversarial training\n",
    "    from vat import VAT\n",
    "    ssl_obj = VAT(alg_cfg[\"eps\"][args.dataset], alg_cfg[\"xi\"], 1)\n",
    "elif args.alg == \"MT\": # mean teacher\n",
    "    from mean_teacher import MT\n",
    "    t_model = wrn.WRN(2, dataset_cfg[\"num_classes\"], transform_fn).to(device)\n",
    "    t_model.load_state_dict(model.state_dict())\n",
    "    ssl_obj = MT(t_model, alg_cfg[\"ema_factor\"])\n",
    "elif args.alg == \"PI\": # PI Model\n",
    "    from pimodel import PiModel\n",
    "    ssl_obj = PiModel()\n",
    "else:\n",
    "    raise ValueError(\"{} is unknown algorithm\".format(args.alg))\n",
    "\n",
    "print()\n",
    "iteration = 0\n",
    "maximum_val_acc = 0\n",
    "s = time.time()\n",
    "for l_data, u_data in zip(l_loader, u_loader):\n",
    "    iteration += 1\n",
    "    l_input, target = l_data\n",
    "    l_input, target = l_input.to(device).float(), target.to(device).long()\n",
    "\n",
    "    if args.alg != \"supervised\": # for ssl algorithm\n",
    "        u_input, dummy_target = u_data\n",
    "        u_input, dummy_target = u_input.to(device).float(), dummy_target.to(device).long()\n",
    "\n",
    "        target = torch.cat([target, dummy_target], 0)\n",
    "        unlabeled_mask = (target == -1).float()\n",
    "\n",
    "        inputs = torch.cat([l_input, u_input], 0)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # ramp up exp(-5(1 - t)^2)\n",
    "        coef = alg_cfg[\"consis_coef\"] * math.exp(-5 * (1 - min(iteration/shared_cfg[\"warmup\"], 1))**2)\n",
    "        ssl_loss = ssl_obj(inputs, outputs.detach(), model, unlabeled_mask) * coef\n",
    "\n",
    "    else:\n",
    "        outputs = model(l_input)\n",
    "        coef = 0\n",
    "        ssl_loss = torch.zeros(1).to(device)\n",
    "\n",
    "    # supervised loss\n",
    "    cls_loss = F.cross_entropy(outputs, target, reduction=\"none\", ignore_index=-1).mean()\n",
    "\n",
    "    loss = cls_loss + ssl_loss\n",
    "\n",
    "    if args.em > 0:\n",
    "        loss -= args.em * ((outputs.softmax(1) * F.log_softmax(outputs, 1)).sum(1) * unlabeled_mask).mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if args.alg == \"MT\" or args.alg == \"ICT\":\n",
    "        # parameter update with exponential moving average\n",
    "        ssl_obj.moving_average(model.parameters())\n",
    "    # display\n",
    "    if iteration == 1 or (iteration % 1000) == 0:\n",
    "        wasted_time = time.time() - s\n",
    "        rest = (shared_cfg[\"iteration\"] - iteration)/100 * wasted_time / 60\n",
    "        print(\"iteration [{}/{}] cls loss : {:.6e}, SSL loss : {:.6e}, coef : {:.5e}, time : {:.3f} iter/sec, rest : {:.3f} min, lr : {}\".format(\n",
    "            iteration, shared_cfg[\"iteration\"], cls_loss.item(), ssl_loss.item(), coef, 100 / wasted_time, rest, optimizer.param_groups[0][\"lr\"]),\n",
    "            \"\\r\")\n",
    "        s = time.time()\n",
    "\n",
    "    # validation\n",
    "    if (iteration % args.validation) == 0 or iteration == shared_cfg[\"iteration\"]:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            print()\n",
    "            print(\"### validation ###\")\n",
    "            sum_acc = 0.\n",
    "            s = time.time()\n",
    "            for j, data in enumerate(val_loader):\n",
    "                input, target = data\n",
    "                input, target = input.to(device).float(), target.to(device).long()\n",
    "\n",
    "                output = model(input)\n",
    "\n",
    "                pred_label = output.max(1)[1]\n",
    "                sum_acc += (pred_label == target).float().sum()\n",
    "                if ((j+1) % 10) == 0:\n",
    "                    d_p_s = 10/(time.time()-s)\n",
    "                    print(\"[{}/{}] time : {:.1f} data/sec, rest : {:.2f} sec\".format(\n",
    "                        j+1, len(val_loader), d_p_s, (len(val_loader) - j-1)/d_p_s\n",
    "                    ), \"\\r\", end=\"\")\n",
    "                    s = time.time()\n",
    "            acc = sum_acc/float(len(val_dataset))\n",
    "            print()\n",
    "            print(\"varidation accuracy : {}\".format(acc))\n",
    "            # test\n",
    "            if maximum_val_acc < acc:\n",
    "                print(\"### test ###\")\n",
    "                maximum_val_acc = acc\n",
    "                sum_acc = 0.\n",
    "                s = time.time()\n",
    "                for j, data in enumerate(test_loader):\n",
    "                    input, target = data\n",
    "                    input, target = input.to(device).float(), target.to(device).long()\n",
    "                    output = model(input)\n",
    "                    pred_label = output.max(1)[1]\n",
    "                    sum_acc += (pred_label == target).float().sum()\n",
    "                    if ((j+1) % 10) == 0:\n",
    "                        d_p_s = 100/(time.time()-s)\n",
    "                        print(\"[{}/{}] time : {:.1f} data/sec, rest : {:.2f} sec\".format(\n",
    "                            j+1, len(test_loader), d_p_s, (len(test_loader) - j-1)/d_p_s\n",
    "                        ), \"\\r\", end=\"\")\n",
    "                        s = time.time()\n",
    "                print()\n",
    "                test_acc = sum_acc / float(len(test_dataset))\n",
    "                print(\"test accuracy : {}\".format(test_acc))\n",
    "                # torch.save(model.state_dict(), os.path.join(args.output, \"best_model.pth\"))\n",
    "        model.train()\n",
    "        s = time.time()\n",
    "    # lr decay\n",
    "    if iteration == shared_cfg[\"lr_decay_iter\"]:\n",
    "        optimizer.param_groups[0][\"lr\"] *= shared_cfg[\"lr_decay_factor\"]\n",
    "\n",
    "print(\"test acc : {}\".format(test_acc))\n",
    "condition[\"test_acc\"] = test_acc.item()\n",
    "\n",
    "exp_name += str(int(time.time())) # unique ID\n",
    "if not os.path.exists(args.output):\n",
    "    os.mkdir(args.output)\n",
    "with open(os.path.join(args.output, exp_name + \".json\"), \"w\") as f:\n",
    "    json.dump(condition, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d343810",
   "metadata": {},
   "outputs": [],
   "source": [
    "* 결과해석:\n",
    "    Mean Teacher모델을 CIFAR-10의 50000개의 데이터를 256의 배치 사이즈로 8800 iteration 훈련시킨 결과 \n",
    "    \"supervised loss : 0.005\", \"unsupervised loss: 0.02\"로 수렴함\n",
    "    \n",
    "    테스트 결과는 0.5929임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3a5f06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f6572d0",
   "metadata": {},
   "source": [
    "### Virtual Adversarial Training(VAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18293445",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.alg = 'VAT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "445c4e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algorithm : VAT\n",
      "maximum iteration : 8800\n",
      "parameters :  {'xi': 1e-06, 'eps': {'cifar10': 6, 'svhn': 1}, 'consis_coef': 0.3, 'lr': 0.003}\n",
      "trainable parameters : 1467610\n"
     ]
    }
   ],
   "source": [
    "if args.alg != \"supervised\":\n",
    "    # batch size = 0.5 x batch size\n",
    "    l_loader = DataLoader(\n",
    "        l_train_dataset, shared_cfg[\"batch_size\"]//2, drop_last=True,\n",
    "        sampler=RandomSampler(len(l_train_dataset), shared_cfg[\"iteration\"] * shared_cfg[\"batch_size\"]//2)\n",
    "    )\n",
    "else:\n",
    "    l_loader = DataLoader(\n",
    "        l_train_dataset, shared_cfg[\"batch_size\"], drop_last=True,\n",
    "        sampler=RandomSampler(len(l_train_dataset), shared_cfg[\"iteration\"] * shared_cfg[\"batch_size\"])\n",
    "    )\n",
    "print(\"algorithm : {}\".format(args.alg))\n",
    "condition[\"algorithm\"] = args.alg\n",
    "exp_name += str(args.alg) + \"_\"\n",
    "\n",
    "u_loader = DataLoader(\n",
    "    u_train_dataset, shared_cfg[\"batch_size\"]//2, drop_last=True,\n",
    "    sampler=RandomSampler(len(u_train_dataset), shared_cfg[\"iteration\"] * shared_cfg[\"batch_size\"]//2)\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(val_dataset, 128, shuffle=False, drop_last=False)\n",
    "test_loader = DataLoader(test_dataset, 128, shuffle=False, drop_last=False)\n",
    "\n",
    "print(\"maximum iteration : {}\".format(min(len(l_loader), len(u_loader))))\n",
    "\n",
    "alg_cfg = config[args.alg]\n",
    "print(\"parameters : \", alg_cfg)\n",
    "condition[\"h_parameters\"] = alg_cfg\n",
    "\n",
    "if args.em > 0:\n",
    "    print(\"entropy minimization : {}\".format(args.em))\n",
    "    exp_name += \"em_\"\n",
    "condition[\"entropy_maximization\"] = args.em\n",
    "\n",
    "model = wrn.WRN(2, dataset_cfg[\"num_classes\"], transform_fn).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=alg_cfg[\"lr\"])\n",
    "\n",
    "trainable_paramters = sum([p.data.nelement() for p in model.parameters()])\n",
    "print(\"trainable parameters : {}\".format(trainable_paramters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d38347ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration [1/8800] cls loss : 1.210762e+00, SSL loss : 8.447826e-06, coef : 2.04169e-03, time : 2291.143 iter/sec, rest : 0.064 min, lr : 0.003 \n",
      "iteration [1000/8800] cls loss : 4.877531e-02, SSL loss : 3.387230e-01, coef : 3.00000e-01, time : 0.596 iter/sec, rest : 218.078 min, lr : 0.0006000000000000001 \n",
      "iteration [2000/8800] cls loss : 1.555471e-02, SSL loss : 2.539856e-01, coef : 3.00000e-01, time : 0.596 iter/sec, rest : 190.102 min, lr : 0.0006000000000000001 \n",
      "iteration [3000/8800] cls loss : 2.724327e-02, SSL loss : 2.493916e-01, coef : 3.00000e-01, time : 0.596 iter/sec, rest : 162.275 min, lr : 0.0006000000000000001 \n",
      "iteration [4000/8800] cls loss : 1.993705e-02, SSL loss : 2.151401e-01, coef : 3.00000e-01, time : 0.595 iter/sec, rest : 134.360 min, lr : 0.0006000000000000001 \n",
      "iteration [5000/8800] cls loss : 7.853318e-03, SSL loss : 2.104976e-01, coef : 3.00000e-01, time : 0.596 iter/sec, rest : 106.228 min, lr : 0.0006000000000000001 \n",
      "iteration [6000/8800] cls loss : 1.117347e-02, SSL loss : 3.346981e-01, coef : 3.00000e-01, time : 0.594 iter/sec, rest : 78.505 min, lr : 0.0006000000000000001 \n",
      "iteration [7000/8800] cls loss : 2.350415e-02, SSL loss : 2.432611e-01, coef : 3.00000e-01, time : 0.597 iter/sec, rest : 50.251 min, lr : 0.0006000000000000001 \n",
      "iteration [8000/8800] cls loss : 5.636553e-03, SSL loss : 2.160951e-01, coef : 3.00000e-01, time : 0.597 iter/sec, rest : 22.325 min, lr : 0.0006000000000000001 \n",
      "\n",
      "### validation ###\n",
      "[40/40] time : 115.4 data/sec, rest : 0.00 sec \n",
      "varidation accuracy : 0.6553999781608582\n",
      "### test ###\n",
      "[70/79] time : 1162.8 data/sec, rest : 0.01 sec \n",
      "test accuracy : 0.6506999731063843\n",
      "test acc : 0.6506999731063843\n"
     ]
    }
   ],
   "source": [
    "if args.alg == \"VAT\": # virtual adversarial training\n",
    "    from vat import VAT\n",
    "    ssl_obj = VAT(alg_cfg[\"eps\"][args.dataset], alg_cfg[\"xi\"], 1)\n",
    "elif args.alg == \"MT\": # mean teacher\n",
    "    from mean_teacher import MT\n",
    "    t_model = wrn.WRN(2, dataset_cfg[\"num_classes\"], transform_fn).to(device)\n",
    "    t_model.load_state_dict(model.state_dict())\n",
    "    ssl_obj = MT(t_model, alg_cfg[\"ema_factor\"])\n",
    "elif args.alg == \"PI\": # PI Model\n",
    "    from pimodel import PiModel\n",
    "    ssl_obj = PiModel()\n",
    "else:\n",
    "    raise ValueError(\"{} is unknown algorithm\".format(args.alg))\n",
    "\n",
    "print()\n",
    "iteration = 0\n",
    "maximum_val_acc = 0\n",
    "s = time.time()\n",
    "for l_data, u_data in zip(l_loader, u_loader):\n",
    "    iteration += 1\n",
    "    l_input, target = l_data\n",
    "    l_input, target = l_input.to(device).float(), target.to(device).long()\n",
    "\n",
    "    if args.alg != \"supervised\": # for ssl algorithm\n",
    "        u_input, dummy_target = u_data\n",
    "        u_input, dummy_target = u_input.to(device).float(), dummy_target.to(device).long()\n",
    "\n",
    "        target = torch.cat([target, dummy_target], 0)\n",
    "        unlabeled_mask = (target == -1).float()\n",
    "\n",
    "        inputs = torch.cat([l_input, u_input], 0)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # ramp up exp(-5(1 - t)^2)\n",
    "        coef = alg_cfg[\"consis_coef\"] * math.exp(-5 * (1 - min(iteration/shared_cfg[\"warmup\"], 1))**2)\n",
    "        ssl_loss = ssl_obj(inputs, outputs.detach(), model, unlabeled_mask) * coef\n",
    "\n",
    "    else:\n",
    "        outputs = model(l_input)\n",
    "        coef = 0\n",
    "        ssl_loss = torch.zeros(1).to(device)\n",
    "\n",
    "    # supervised loss\n",
    "    cls_loss = F.cross_entropy(outputs, target, reduction=\"none\", ignore_index=-1).mean()\n",
    "\n",
    "    loss = cls_loss + ssl_loss\n",
    "\n",
    "    if args.em > 0:\n",
    "        loss -= args.em * ((outputs.softmax(1) * F.log_softmax(outputs, 1)).sum(1) * unlabeled_mask).mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if args.alg == \"MT\" or args.alg == \"ICT\":\n",
    "        # parameter update with exponential moving average\n",
    "        ssl_obj.moving_average(model.parameters())\n",
    "    # display\n",
    "    if iteration == 1 or (iteration % 1000) == 0:\n",
    "        wasted_time = time.time() - s\n",
    "        rest = (shared_cfg[\"iteration\"] - iteration)/100 * wasted_time / 60\n",
    "        print(\"iteration [{}/{}] cls loss : {:.6e}, SSL loss : {:.6e}, coef : {:.5e}, time : {:.3f} iter/sec, rest : {:.3f} min, lr : {}\".format(\n",
    "            iteration, shared_cfg[\"iteration\"], cls_loss.item(), ssl_loss.item(), coef, 100 / wasted_time, rest, optimizer.param_groups[0][\"lr\"]),\n",
    "            \"\\r\")\n",
    "        s = time.time()\n",
    "\n",
    "    # validation\n",
    "    if (iteration % args.validation) == 0 or iteration == shared_cfg[\"iteration\"]:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            print()\n",
    "            print(\"### validation ###\")\n",
    "            sum_acc = 0.\n",
    "            s = time.time()\n",
    "            for j, data in enumerate(val_loader):\n",
    "                input, target = data\n",
    "                input, target = input.to(device).float(), target.to(device).long()\n",
    "\n",
    "                output = model(input)\n",
    "\n",
    "                pred_label = output.max(1)[1]\n",
    "                sum_acc += (pred_label == target).float().sum()\n",
    "                if ((j+1) % 10) == 0:\n",
    "                    d_p_s = 10/(time.time()-s)\n",
    "                    print(\"[{}/{}] time : {:.1f} data/sec, rest : {:.2f} sec\".format(\n",
    "                        j+1, len(val_loader), d_p_s, (len(val_loader) - j-1)/d_p_s\n",
    "                    ), \"\\r\", end=\"\")\n",
    "                    s = time.time()\n",
    "            acc = sum_acc/float(len(val_dataset))\n",
    "            print()\n",
    "            print(\"varidation accuracy : {}\".format(acc))\n",
    "            # test\n",
    "            if maximum_val_acc < acc:\n",
    "                print(\"### test ###\")\n",
    "                maximum_val_acc = acc\n",
    "                sum_acc = 0.\n",
    "                s = time.time()\n",
    "                for j, data in enumerate(test_loader):\n",
    "                    input, target = data\n",
    "                    input, target = input.to(device).float(), target.to(device).long()\n",
    "                    output = model(input)\n",
    "                    pred_label = output.max(1)[1]\n",
    "                    sum_acc += (pred_label == target).float().sum()\n",
    "                    if ((j+1) % 10) == 0:\n",
    "                        d_p_s = 100/(time.time()-s)\n",
    "                        print(\"[{}/{}] time : {:.1f} data/sec, rest : {:.2f} sec\".format(\n",
    "                            j+1, len(test_loader), d_p_s, (len(test_loader) - j-1)/d_p_s\n",
    "                        ), \"\\r\", end=\"\")\n",
    "                        s = time.time()\n",
    "                print()\n",
    "                test_acc = sum_acc / float(len(test_dataset))\n",
    "                print(\"test accuracy : {}\".format(test_acc))\n",
    "                # torch.save(model.state_dict(), os.path.join(args.output, \"best_model.pth\"))\n",
    "        model.train()\n",
    "        s = time.time()\n",
    "    # lr decay\n",
    "    if iteration == shared_cfg[\"lr_decay_iter\"]:\n",
    "        optimizer.param_groups[0][\"lr\"] *= shared_cfg[\"lr_decay_factor\"]\n",
    "\n",
    "print(\"test acc : {}\".format(test_acc))\n",
    "condition[\"test_acc\"] = test_acc.item()\n",
    "\n",
    "exp_name += str(int(time.time())) # unique ID\n",
    "if not os.path.exists(args.output):\n",
    "    os.mkdir(args.output)\n",
    "with open(os.path.join(args.output, exp_name + \".json\"), \"w\") as f:\n",
    "    json.dump(condition, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f2caf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "* 결과해석:\n",
    "    VAT모델을 CIFAR-10의 50000개의 데이터를 256의 배치 사이즈로 8800 iteration 훈련시킨 결과 \n",
    "    \"supervised loss : 0.005\", \"unsupervised loss: 0.2\"로 수렴함\n",
    "    \n",
    "    테스트 결과는 0.65임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79762bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "* 전체 결과해석:\n",
    "    먼저, CIFAR-10 데이터셋을 활용하여 동일한 파라미터로 실험 후, TEST한 정확도 결과값은 아래와 같이,\n",
    "    VAT, MT, Pi모델 순이었다.\n",
    "    \n",
    "    1. VAT: 65.07% (0.597 iter/sec)\n",
    "    2. Mean Teacher: 59.29% (0.759 iter/sec)\n",
    "    3. Pi-Model: 59.14% (0.886 iter/sec)\n",
    "        \n",
    "    3모델의 trainable parameter는 1467610로 고정하였으므로, 작은 노이즈에 취약하지 않은 강건한 모델인\n",
    "    VAT의 성능이 가장 높은 것을 볼 수 있다.\n",
    "    \n",
    "    teacher와 student를 분리하여 학습한 mean teacher는 속도와 성능 면에서 pi-model에 비해 증가하였으나, \n",
    "    VAT처럼 큰 변화는 없었다. 일관성 제약의 접근을 고려하였을 때, 이미지들의 분류성능을 가장 높일 수 있는 \n",
    "    준지도 학습 모델은 VAT인 것을 속도와 성능면에서 모두 확인 할 수 있었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a17d3d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a67c8df",
   "metadata": {},
   "source": [
    "## MNIST 데이터셋 활용하여 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f9a544",
   "metadata": {},
   "source": [
    "## 1.Temporal Ensemble(2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2880028",
   "metadata": {},
   "outputs": [],
   "source": [
    "* Temporal Ensemble(2016): 파이모델의 한계점이 ‘single network＇이었기 때문에,\n",
    "    Multiple previous network evaluation의 예측 값들을 앙상블 prediction으로 취합함\n",
    "    \n",
    "    Teacher 모델의 Output이 불안정(noisy)하므로, EMA로 누적해 안정성을 높임\n",
    "    \n",
    "    (단점) Epoch마다 데이터 Z를 보관할 용량이 필요함 <-- 누적된 벡터값이 Z에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4942be65",
   "metadata": {},
   "outputs": [],
   "source": [
    "* 출처: https://github.com/ferretj/temporal-ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05088334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils as utils\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13ab362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_dataset(root, transform):\n",
    "\n",
    "    # load train data\n",
    "    train_dataset = datasets.MNIST(\n",
    "        root=root,\n",
    "        train=True,\n",
    "        transform=transform,\n",
    "        download=True)\n",
    "\n",
    "    # load test data\n",
    "    test_dataset = datasets.MNIST(\n",
    "        root=root,\n",
    "        train=False,\n",
    "        transform=transform, download=True)\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "def sample_train(train_dataset, test_dataset, batch_size, k, n_classes, seed, shuffle_train=False, return_idx=True):\n",
    "    '''Randomly form unlabeled data in training dataset'''\n",
    "\n",
    "    n = len(train_dataset)  # dataset size\n",
    "    rrng = np.random.RandomState(seed) # seed \n",
    "    indices = torch.zeros(k)  # indices of keep labeled data\n",
    "    others = torch.zeros(n - k)  # indices of unlabeled data\n",
    "    card = k // n_classes\n",
    "    cpt = 0\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        class_items = (train_dataset.train_labels == i).nonzero()  # indices of samples with label i\n",
    "        n_class = len(class_items)  # number of samples with label i\n",
    "        rd = rrng.permutation(np.arange(n_class))  # shuffle them\n",
    "        indices[i * card: (i+1) * card] = torch.squeeze(class_items[rd[:card]])\n",
    "        others[cpt: cpt+n_class-card] = torch.squeeze(class_items[rd[card:]])\n",
    "        cpt += (n_class-card)\n",
    "\n",
    "    # tensor as indices must be long, byte or bool\n",
    "    others = others.long()\n",
    "    train_dataset.train_labels[others] = -1\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               num_workers=2,\n",
    "                                               shuffle=shuffle_train)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              num_workers=2,\n",
    "                                              shuffle=False)\n",
    "\n",
    "    if return_idx:\n",
    "        return train_loader, test_loader, indices\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d62607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNoise(nn.Module):\n",
    "    \"\"\"데이터에 noise 추가\"\"\"\n",
    "    def __init__(self, batch_size, input_shape, std):\n",
    "        super(GaussianNoise, self).__init__()\n",
    "        self.shape = (batch_size, ) + input_shape\n",
    "        self.std = std\n",
    "        self.noise = torch.zeros(self.shape).cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.noise.normal_(mean=0, std=self.std)\n",
    "        # print(self.noise.shape)\n",
    "\n",
    "        return x + self.noise\n",
    "\n",
    "    def temporal_losses(out1, out2, w, labels):\n",
    "        # output1: current output\n",
    "        # output2: temporal output\n",
    "        # w: weight for summation loss\n",
    "\n",
    "        \"ensemble output과 current output을 통해 supervised, unsupervised loss 및 total loss를 계산함\"\n",
    "\n",
    "        sup_loss, nbsup = GaussianNoise.masked_crossentropy(out1, labels)\n",
    "        unsup_loss = GaussianNoise.mse_loss(out1, out2)\n",
    "        total_loss = sup_loss + w * unsup_loss\n",
    "\n",
    "        return total_loss, sup_loss, unsup_loss, nbsup\n",
    "\n",
    "    def mse_loss(out1, out2):\n",
    "        \"current output, ensemble output 간의 mean difference: unsupervised loss\"\n",
    "        quad_diff = torch.sum((F.softmax(out1, dim=1) - F.softmax(out2, dim=1)) ** 2)\n",
    "\n",
    "        return quad_diff / out1.data.nelement()\n",
    "\n",
    "    def masked_crossentropy(out, labels):\n",
    "        \"labeld된 data에 한해서 crossentropy loss를 계산함\"\n",
    "        cond = (labels >= 0)\n",
    "        nnz = torch.nonzero(cond)  # array of labeled sample index\n",
    "        nbsup = len(nnz)  # number of supervised samples\n",
    "        # check if labeled samples in batch, return 0 if none\n",
    "        if nbsup > 0:\n",
    "            # select lines in out with label\n",
    "            masked_outputs = torch.index_select(out, 0, nnz.view(nbsup))\n",
    "            masked_labels = labels[cond]\n",
    "            loss = F.cross_entropy(masked_outputs, masked_labels)\n",
    "            return loss, nbsup\n",
    "        loss = torch.tensor([0.], requires_grad=False).cuda()\n",
    "        return loss, 0\n",
    "\n",
    "    def weight_scheduler(epoch, max_epochs, max_val, mult, n_labeled, n_samples):\n",
    "        \"epoch이 지남에 따라 weight를 조정함\"\n",
    "        max_val = max_val * (float(n_labeled) / n_samples)\n",
    "        return GaussianNoise.ramp_up(epoch, max_epochs, max_val, mult)\n",
    "\n",
    "    def ramp_up(epoch, max_epochs, max_val, mult):\n",
    "        \"weight를 조정하며 첫 epoch에는 0을 사용함\"\n",
    "        if epoch == 0:\n",
    "            return 0.\n",
    "        elif epoch >= max_epochs:\n",
    "            return max_val\n",
    "        return max_val * np.exp(-mult * (1. - float(epoch) / max_epochs) ** 2)\n",
    "    \n",
    "    def calc_metrics(model, loader):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, (samples, labels) in enumerate(loader):\n",
    "            samples = samples.cuda()\n",
    "            labels = labels.requires_grad_(False).cuda()\n",
    "            outputs = model(samples)\n",
    "            _, predicted = torch.max(outputs.detach(), 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.detach().view_as(predicted)).sum()\n",
    "        acc = 100 * float(correct) / total\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1442e7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, batch_size, std, input_shape=(1, 28, 28), p=0.5, fm1=16, fm2=32):\n",
    "        super(CNN, self).__init__()\n",
    "        self.std = std\n",
    "        self.p = p\n",
    "        self.fm1 = fm1\n",
    "        self.fm2 = fm2\n",
    "        self.input_shape = input_shape\n",
    "        self.conv_block1 = nn.Sequential(nn.Conv2d(1, self.fm1, 3, stride=1, padding=1),\n",
    "                                        nn.BatchNorm2d(self.fm1), \n",
    "                                        nn.ReLU(),\n",
    "                                        nn.MaxPool2d(3, stride=2, padding=1)\n",
    "                                      )\n",
    "        \n",
    "        self.conv_block2 = nn.Sequential(nn.Conv2d(self.fm1, self.fm2, 3, stride=1, padding=1),\n",
    "                                        nn.BatchNorm2d(self.fm2), \n",
    "                                        nn.ReLU(),\n",
    "                                        nn.MaxPool2d(3, stride=2, padding=1)\n",
    "                                      )\n",
    "        self.drop = nn.Dropout(self.p)\n",
    "        self.fc = nn.Linear(self.fm2 * 7 * 7, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            b = x.size(0)\n",
    "            gn = GaussianNoise(b, self.input_shape, self.std)\n",
    "            x = gn(x)\n",
    "\n",
    "        # first block\n",
    "        x = self.conv_block1(x)\n",
    "        \n",
    "        # second block\n",
    "        x = self.conv_block2(x)\n",
    "\n",
    "        # classifier\n",
    "        x = x.view(-1, self.fm2 * 7 * 7)\n",
    "        x = self.fc(self.drop(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d10a77f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 학습\n",
    "\n",
    "def train(model, train_loader, val_loader ,seed, k, alpha, lr, num_epochs, batch_size, ntrain,n_classes=10, max_epochs=80, max_val=1.):\n",
    "\n",
    "    # build model and feed to GPU\n",
    "    model.cuda()\n",
    "\n",
    "    # setup param optimization\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.99))\n",
    "\n",
    "    # model.train()\n",
    "    \n",
    "    # 첫 ensemble ouput은 모두 0\n",
    "    Z = torch.zeros(ntrain, n_classes).float().cuda()  # intermediate values\n",
    "    z = torch.zeros(ntrain, n_classes).float().cuda()  # temporal outputs\n",
    "    outputs = torch.zeros(ntrain, n_classes).float().cuda()  # current outputs\n",
    "\n",
    "    losses = []\n",
    "    suplosses = []\n",
    "    unsuplosses = []\n",
    "    best_loss = 30.0\n",
    "    for epoch in range(num_epochs):\n",
    "        start_t = time.time()\n",
    "        if epoch==0 or epoch%10==0 or epoch==49:\n",
    "            print('\\nEpoch: {}'.format(epoch+1))\n",
    "        model.train()\n",
    "        # evaluate unsupervised cost weight\n",
    "        w = GaussianNoise.weight_scheduler(epoch, max_epochs, max_val, 5, k, 60000)\n",
    "\n",
    "        w = torch.tensor(w, requires_grad=False).cuda()\n",
    "        if epoch==0 or epoch%10==0 or epoch==49:\n",
    "            print('---------------------')\n",
    "\n",
    "        # targets change only once per epoch\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            #print(i)\n",
    "            batch_size = images.size(0)  # retrieve batch size again cause drop last is false\n",
    "            images = images.cuda()\n",
    "            labels = labels.requires_grad_(False).cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = model(images)\n",
    "            # 현재 batch에 맞는 ensemble 결과들을 가져옴\n",
    "            zcomp = z[i * batch_size: (i+1) * batch_size]\n",
    "            zcomp.requires_grad_(False)\n",
    "            loss, suploss, unsuploss, nbsup = GaussianNoise.temporal_losses(out, zcomp, w, labels)\n",
    "\n",
    "            # save outputs\n",
    "            outputs[i * batch_size: (i+1) * batch_size] = out.clone().detach()\n",
    "            losses.append(loss.item())\n",
    "            suplosses.append(nbsup * suploss.item())\n",
    "            unsuplosses.append(unsuploss.item())\n",
    "\n",
    "            # backprop\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        loss_mean = np.mean(losses)\n",
    "        supl_mean = np.mean(suplosses)\n",
    "        unsupl_mean = np.mean(unsuplosses)\n",
    "        if epoch==0 or epoch%10==0 or epoch==49:\n",
    "            print('Epoch [%d/%d], Loss: %.6f, Supervised Loss: %.6f, Unsupervised Loss: %.6f, Time: %.2f' % \n",
    "                  (epoch + 1, num_epochs, float(loss_mean), float(supl_mean), float(unsupl_mean), time.time()-start_t))\n",
    "\n",
    "        # model의 outputs을 가중평균을 이용해 ensemble outputs으로 update 함\n",
    "        Z = alpha * Z + (1. - alpha) * outputs\n",
    "        z = Z * (1. / (1. - alpha ** (epoch + 1)))\n",
    "\n",
    "        if loss_mean < best_loss:\n",
    "            best_loss = loss_mean\n",
    "            torch.save({'state_dict': model.state_dict()}, 'model_best.pth')\n",
    "\n",
    "        model.eval()\n",
    "        acc = GaussianNoise.calc_metrics(model, val_loader)\n",
    "        if epoch==0 or epoch%10==0 or epoch==49:\n",
    "            print('Acc : %.2f' % acc)\n",
    "\n",
    "def evaluation(model, loader):\n",
    "\n",
    "    # test best model\n",
    "    checkpoint = torch.load('model_best.pth')\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (samples, labels) in enumerate(loader):\n",
    "        samples = samples.cuda()\n",
    "        labels = labels.requires_grad_(False).cuda()\n",
    "        outputs = model(samples)\n",
    "        _, predicted = torch.max(outputs.detach(), 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.detach().view_as(predicted)).sum()\n",
    "    acc = 100 * float(correct) / total\n",
    "    print('Acc (best model): %.2f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "28f15c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global vars\n",
    "n_exp = 1 # number of experiments, try 5 different seed\n",
    "k = 100 # keep k labeled data in whole training set, other without label\n",
    "\n",
    "# dataset vars\n",
    "m = 0.1307\n",
    "s = 0.3081\n",
    "\n",
    "# model vars\n",
    "drop = 0.5 # dropout probability\n",
    "std = 0.15 # std of gaussian noise\n",
    "fm1 = 32 # channels of the first conv\n",
    "fm2 = 64 # channels of the second conv\n",
    "w_norm = True\n",
    "\n",
    "# optim vars\n",
    "learning_rate = 0.002\n",
    "beta2 = 0.99 # second momentum for Adam\n",
    "num_epochs = 50\n",
    "batch_size = 64\n",
    "\n",
    "# temporal ensembling vars\n",
    "alpha = 0.6 # ensembling momentum\n",
    "data_norm = 'channelwise' # image normalization\n",
    "divide_by_bs = False # whether we divide supervised cost by batch_size\n",
    "\n",
    "# RNG\n",
    "rng = np.random.RandomState(42)\n",
    "seeds = [rng.randint(200) for _ in range(n_exp)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ba154b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n",
      "---------------------\n",
      "Epoch [1/50], Loss: 0.761350, Supervised Loss: 0.790792, Unsupervised Loss: 0.065815, Time: 4.39\n",
      "Acc : 31.96\n",
      "\n",
      "Epoch: 11\n",
      "---------------------\n",
      "Epoch [11/50], Loss: 0.144925, Supervised Loss: 0.149759, Unsupervised Loss: 0.037651, Time: 4.31\n",
      "Acc : 86.34\n",
      "\n",
      "Epoch: 21\n",
      "---------------------\n",
      "Epoch [21/50], Loss: 0.080538, Supervised Loss: 0.083150, Unsupervised Loss: 0.027056, Time: 4.32\n",
      "Acc : 92.20\n",
      "\n",
      "Epoch: 31\n",
      "---------------------\n",
      "Epoch [31/50], Loss: 0.054949, Supervised Loss: 0.056718, Unsupervised Loss: 0.021271, Time: 4.26\n",
      "Acc : 94.06\n",
      "\n",
      "Epoch: 41\n",
      "---------------------\n",
      "Epoch [41/50], Loss: 0.042135, Supervised Loss: 0.043472, Unsupervised Loss: 0.017923, Time: 4.33\n",
      "Acc : 94.83\n",
      "\n",
      "Epoch: 50\n",
      "---------------------\n",
      "Epoch [50/50], Loss: 0.035941, Supervised Loss: 0.037041, Unsupervised Loss: 0.016183, Time: 4.50\n",
      "Acc : 95.20\n",
      "Acc (best model): 95.20\n"
     ]
    }
   ],
   "source": [
    "# cfg = vars(config)\n",
    "\n",
    "# prepare data\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(m,s)])\n",
    "train_dataset, val_dataset = mnist_dataset(root='~/datasets/MNIST', transform=transform)\n",
    "ntrain = len(train_dataset)\n",
    "\n",
    "\n",
    "for i in range(n_exp):\n",
    "    model = CNN(batch_size, std, fm1=fm1, fm2=fm2).cuda()\n",
    "    seed = seeds[i]\n",
    "    train_loader, val_loader, indices = sample_train(train_dataset, val_dataset, batch_size=batch_size,\n",
    "                                                 k=k, n_classes=10, seed=seed, shuffle_train=False)\n",
    "    train(model, train_loader, val_loader,seed, k, alpha, learning_rate,\n",
    "         num_epochs, batch_size, ntrain)\n",
    "    evaluation(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6430b70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "*결과해석: Temporal Ensembling은 앞서 언급한 것처럼, \n",
    "    파이모델처럼 single network가 아닌 mulitple network evaluation의 예측 값들을 앙상블하여 예측값으로 사용함\n",
    "    \n",
    "    따라서, 파이모델에 비해 좀 더 robust한 값이 결과값으로 나올 수 있게 하였다.\n",
    "    \n",
    "    Temporal Ensembling을 MNIST의 60000개의 데이터를 64의 배치 사이즈로 50epoch 훈련시킨 결과,\n",
    "    \"supervised loss : 0.037041\", \"unsupervised loss: 0.035941\"로 수렴함\n",
    "    \n",
    "    테스트 결과는 0.9520임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccd31b8",
   "metadata": {},
   "source": [
    "## 2. Mean Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1664ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "* 출처: https://github.com/shenkev/Pytorch-Mean-Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e43829ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.315418\tUn_loss: 0.004357\tS_loss: 2.311061\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.136551\tUn_loss: 0.030252\tS_loss: 0.106298\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.120265\tUn_loss: 0.017260\tS_loss: 0.103005\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.051837\tUn_loss: 0.004423\tS_loss: 0.047414\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.015498\tUn_loss: 0.006576\tS_loss: 0.008921\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.041722\tUn_loss: 0.003392\tS_loss: 0.038331\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.016616\tUn_loss: 0.002966\tS_loss: 0.013650\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.015772\tUn_loss: 0.005802\tS_loss: 0.009969\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.051772\tUn_loss: 0.007063\tS_loss: 0.044709\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.008176\tUn_loss: 0.003209\tS_loss: 0.004967\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.044865\tUn_loss: 0.003069\tS_loss: 0.041796\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.009619\tUn_loss: 0.005116\tS_loss: 0.004504\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.013999\tUn_loss: 0.003053\tS_loss: 0.010946\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.018161\tUn_loss: 0.003502\tS_loss: 0.014659\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.023307\tUn_loss: 0.001739\tS_loss: 0.021569\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.032441\tUn_loss: 0.008640\tS_loss: 0.023801\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.009311\tUn_loss: 0.001158\tS_loss: 0.008152\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.004888\tUn_loss: 0.000911\tS_loss: 0.003977\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.011637\tUn_loss: 0.001731\tS_loss: 0.009906\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.084157\tUn_loss: 0.003609\tS_loss: 0.080548\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.016889\tUn_loss: 0.003864\tS_loss: 0.013025\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.009852\tUn_loss: 0.004572\tS_loss: 0.005280\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.007176\tUn_loss: 0.003364\tS_loss: 0.003812\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.006558\tUn_loss: 0.003906\tS_loss: 0.002653\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.023794\tUn_loss: 0.019106\tS_loss: 0.004688\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.011000\tUn_loss: 0.008621\tS_loss: 0.002378\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.026127\tUn_loss: 0.016263\tS_loss: 0.009864\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.014467\tUn_loss: 0.009355\tS_loss: 0.005112\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.011717\tUn_loss: 0.006834\tS_loss: 0.004883\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.004172\tUn_loss: 0.000766\tS_loss: 0.003406\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 0.013614\tUn_loss: 0.009304\tS_loss: 0.004311\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 0.009487\tUn_loss: 0.001135\tS_loss: 0.008353\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 0.021979\tUn_loss: 0.019233\tS_loss: 0.002746\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 0.004937\tUn_loss: 0.002272\tS_loss: 0.002665\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 0.004305\tUn_loss: 0.002720\tS_loss: 0.001585\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 0.003854\tUn_loss: 0.001351\tS_loss: 0.002503\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 0.003159\tUn_loss: 0.000254\tS_loss: 0.002906\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 0.005259\tUn_loss: 0.001949\tS_loss: 0.003311\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 0.004023\tUn_loss: 0.002611\tS_loss: 0.001412\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 0.003014\tUn_loss: 0.002236\tS_loss: 0.000778\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 0.006090\tUn_loss: 0.004489\tS_loss: 0.001600\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 0.004932\tUn_loss: 0.001068\tS_loss: 0.003864\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 0.011335\tUn_loss: 0.000775\tS_loss: 0.010560\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 0.008701\tUn_loss: 0.000914\tS_loss: 0.007788\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 0.003864\tUn_loss: 0.001427\tS_loss: 0.002437\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 0.004826\tUn_loss: 0.002566\tS_loss: 0.002260\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 0.002648\tUn_loss: 0.001297\tS_loss: 0.001351\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 0.011124\tUn_loss: 0.008491\tS_loss: 0.002633\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 0.013877\tUn_loss: 0.013391\tS_loss: 0.000485\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 0.004426\tUn_loss: 0.002754\tS_loss: 0.001672\n",
      "\n",
      "Test set: Average loss: 0.0189, Accuracy: 9938/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4 * 4 * 50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "def train(args, model, mean_teacher, device, train_loader, test_loader, optimizer, epoch):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        ########################### CODE CHANGE HERE ######################################\n",
    "        # forward pass with mean teacher\n",
    "        # torch.no_grad() prevents gradients from being passed into mean teacher model\n",
    "        with torch.no_grad():\n",
    "            mean_t_output = mean_teacher(data)\n",
    "\n",
    "        ########################### CODE CHANGE HERE ######################################\n",
    "        # consistency loss (example with MSE, you can change)\n",
    "        const_loss = F.mse_loss(output, mean_t_output)\n",
    "\n",
    "        ########################### CODE CHANGE HERE ######################################\n",
    "        # set the consistency weight (should schedule)\n",
    "        weight = 0.2\n",
    "        loss = F.nll_loss(output, target) + weight*const_loss\n",
    "        unsupervised_loss = weight*const_loss\n",
    "        supervised_loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        ########################### CODE CHANGE HERE ######################################\n",
    "        # update mean teacher, (should choose alpha somehow)\n",
    "        # Use the true average until the exponential average is more correct\n",
    "        alpha = 0.95\n",
    "        for mean_param, param in zip(mean_teacher.parameters(), model.parameters()):\n",
    "            mean_param.data.mul_(alpha).add_(1 - alpha, param.data)\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tUn_loss: {:.6f}\\tS_loss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item(), unsupervised_loss.item(), supervised_loss.item()))\n",
    "\n",
    "\n",
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    model.train()\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=50, metavar='N',\n",
    "                        help='number of epochs to train (default: 10)')\n",
    "    parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                        help='learning rate (default: 0.01)')\n",
    "    parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                        help='SGD momentum (default: 0.5)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=60000, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "\n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "#     args = parser.parse_args()\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])),\n",
    "        batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    model = Net().to(device)\n",
    "    ########################### CODE CHANGE HERE ######################################\n",
    "    # initialize mean teacher\n",
    "    mean_teacher = Net().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, mean_teacher, device, train_loader, test_loader, optimizer, epoch)\n",
    "\n",
    "#     test(args, model, device, test_loader)\n",
    "    test(args, mean_teacher, device, test_loader)\n",
    "\n",
    "    if (args.save_model):\n",
    "        torch.save(model.state_dict(), \"mnist_mean_teacher.pt\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa39b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "* 결과해석:\n",
    "    Mean Teacher모델을 MNIST의 60000개의 데이터를 64의 배치 사이즈로 50epoch 훈련시킨 결과,\n",
    "    \"supervised loss : 0.0016\", \"unsupervised loss: 0.002\"로 수렴함\n",
    "    \n",
    "    테스트 결과는 0.9938임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd38e7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "* 전체 결과해석:\n",
    "    먼저, MNIST 데이터셋을 활용하여 동일한 파라미터로 실험 후, TEST한 정확도 결과값은 아래와 같이,\n",
    "    MT, Temporal Ensembling 모델 순이었다.\n",
    "    \n",
    "    1. Mean Teacher: 99.38%\n",
    "    2. Temporal Ensembling: 95.20%\n",
    "        \n",
    "    Temporal Ensemble에선 output이 불안정하여 EMA(Exponential moving average)로 누적하여 안정성을 높인 것을 \n",
    "    택하였지만, mean teacher에서는 teacher와 student를 각각 지정해 'student의 가중치를 EMA하여 teacher에 사용'하였다.\n",
    "    \n",
    "    결과에서도 볼 수 있듯이, Temporal Ensembling의 주요 기법인 output의 평균값을 적용하는 것보다,\n",
    "    Mean teacher처럼, teacher와 student를 지정하여서 학습하게 하는 것이 메모리의 부담도 적고 속도와 성능면에서\n",
    "    뛰어난 것을 알 수 있었다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jff(python=3.7)",
   "language": "python",
   "name": "jff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
